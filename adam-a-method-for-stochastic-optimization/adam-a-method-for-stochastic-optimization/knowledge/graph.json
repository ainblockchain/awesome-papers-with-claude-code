{
  "nodes": [
    {
      "id": "sgd",
      "name": "Stochastic Gradient Descent (SGD)",
      "type": "optimization",
      "level": "foundational",
      "description": "The foundational optimization algorithm that updates parameters by taking steps proportional to negative gradients computed on mini-batches. SGD is the basis for all modern deep learning optimizers.",
      "key_ideas": ["Mini-batch gradient computation", "Parameter update rule: θ = θ - α∇L", "Stochastic approximation of true gradient"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "momentum",
      "name": "Momentum",
      "type": "optimization",
      "level": "foundational",
      "description": "An optimizer variant that maintains a velocity vector (exponential moving average of gradients) to accelerate convergence in consistent gradient directions and dampen oscillations.",
      "key_ideas": ["Velocity accumulation: v = βv + ∇L", "Acceleration in consistent directions", "Damping of gradient noise"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "adagrad",
      "name": "AdaGrad",
      "type": "optimization",
      "level": "foundational",
      "description": "An adaptive learning rate method that divides the learning rate by the square root of accumulated squared gradients. Performs well on sparse data but suffers from monotonically decreasing learning rates.",
      "key_ideas": ["Per-parameter adaptive learning rates", "Accumulated squared gradients", "Better performance on sparse gradients"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "rmsprop",
      "name": "RMSprop",
      "type": "optimization",
      "level": "foundational",
      "description": "An adaptive learning rate method using exponential moving average of squared gradients instead of cumulative sum. Maintains adaptive scaling without the monotonic decay problem of AdaGrad.",
      "key_ideas": ["Exponential moving average of squared gradients", "Per-parameter learning rate scaling", "Solves AdaGrad's vanishing learning rate issue"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "first_moment",
      "name": "First Moment Estimate",
      "type": "technique",
      "level": "intermediate",
      "description": "An exponential moving average of gradients that estimates the mean direction of gradients. In Adam, this provides momentum-like behavior.",
      "key_ideas": ["Exponential moving average: m = β₁m + (1-β₁)∇L", "Approximates first moment of gradient distribution", "Provides momentum effect"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "second_moment",
      "name": "Second Moment Estimate",
      "type": "technique",
      "level": "intermediate",
      "description": "An exponential moving average of squared gradients that estimates variance. Enables per-parameter adaptive learning rates based on recent gradient magnitudes.",
      "key_ideas": ["Exponential moving average: v = β₂v + (1-β₂)∇L²", "Estimates second moment (uncentered variance)", "Per-parameter learning rate adaptation"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "bias_correction",
      "name": "Bias Correction",
      "type": "technique",
      "level": "intermediate",
      "description": "A correction mechanism for moment estimates during early training. Since moments are initialized to zero, bias correction scales estimates by (1-βᵗ).",
      "key_ideas": ["Corrected first moment: m̂ = m / (1 - β₁ᵗ)", "Corrected second moment: v̂ = v / (1 - β₂ᵗ)", "Ensures stable estimates from iteration 1"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "adaptive_lr",
      "name": "Adaptive Learning Rates",
      "type": "optimization",
      "level": "intermediate",
      "description": "Per-parameter learning rates that automatically adjust based on gradient history. Different parameters get different effective learning rates, improving convergence.",
      "key_ideas": ["Learning rate = α / (sqrt(v) + ε)", "Parameters with large gradients get smaller steps", "Parameters with small gradients get larger steps"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "adamax",
      "name": "AdaMax (Infinity Norm Variant)",
      "type": "variant",
      "level": "advanced",
      "description": "A variant of Adam that uses the infinity norm (max) instead of the L2 norm for the second moment. Often works better on very noisy objectives.",
      "key_ideas": ["Uses max of gradient magnitudes instead of RMS", "u = max(β₂·u, |∇L|)", "More robust to extreme gradient values"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "convergence_analysis",
      "name": "Convergence Analysis",
      "type": "theory",
      "level": "advanced",
      "description": "Theoretical proof that Adam converges to a stationary point under certain conditions. The paper provides regret bounds showing O(1/√T) regret.",
      "key_ideas": ["Convergence to stationary points", "Regret analysis for convex objectives", "Proof handles non-stationary and sparse gradients"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 0.9
    },
    {
      "id": "hyperparameter_tuning",
      "name": "Hyperparameter Tuning",
      "type": "optimization",
      "level": "intermediate",
      "description": "The process of selecting learning rate (α), decay rates (β₁, β₂), and epsilon (ε) values. Adam requires less tuning than SGD due to adaptive nature.",
      "key_ideas": ["Learning rate α: typically 0.001", "β₁: 0.9 (first moment decay)", "β₂: 0.999 (second moment decay)"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "gradient_noise",
      "name": "Handling Gradient Noise",
      "type": "application",
      "level": "intermediate",
      "description": "Techniques for managing high-variance gradients common in stochastic optimization. Adam's dual moment tracking helps stabilize learning.",
      "key_ideas": ["Gradient variance from mini-batch sampling", "Momentum smooths noisy updates", "Adaptive scaling reduces impact of outliers"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "sparse_gradients",
      "name": "Sparse Gradients",
      "type": "application",
      "level": "intermediate",
      "description": "Gradients where most elements are zero, common in NLP and recommendation systems. Adam handles these better than SGD.",
      "key_ideas": ["Many zero gradient entries", "Parameters with non-zero gradients need different rates", "Adam naturally suited for sparse problems"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "memory_efficiency",
      "name": "Memory Efficiency",
      "type": "component",
      "level": "intermediate",
      "description": "Adam's memory requirements are modest: stores only two auxiliary variables per parameter. Practical for large-scale deep learning.",
      "key_ideas": ["Stores only two moment estimates per parameter", "Linear memory overhead", "Practical for modern deep networks"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "computational_efficiency",
      "name": "Computational Efficiency",
      "type": "component",
      "level": "intermediate",
      "description": "Adam's per-iteration cost is similar to SGD: only element-wise operations. No matrix operations or hessian computation needed.",
      "key_ideas": ["O(d) time complexity per iteration", "Only element-wise operations", "Readily parallelizable on modern hardware"],
      "code_refs": [],
      "paper_ref": "Kingma & Ba, 2014 — Adam: A Method for Stochastic Optimization",
      "first_appeared": null,
      "confidence": 1.0
    }
  ],
  "edges": [
    {"source": "sgd", "target": "momentum", "relationship": "evolves_to", "weight": 1.0, "description": "Momentum extends SGD by adding gradient history tracking"},
    {"source": "sgd", "target": "adagrad", "relationship": "variant_of", "weight": 1.0, "description": "AdaGrad adapts SGD with per-parameter learning rates"},
    {"source": "adagrad", "target": "rmsprop", "relationship": "evolves_to", "weight": 1.0, "description": "RMSprop improves upon AdaGrad by using exponential moving average"},
    {"source": "momentum", "target": "first_moment", "relationship": "builds_on", "weight": 1.0, "description": "First moment in Adam generalizes momentum concept"},
    {"source": "rmsprop", "target": "second_moment", "relationship": "builds_on", "weight": 1.0, "description": "Second moment in Adam extends RMSprop's adaptive scaling"},
    {"source": "first_moment", "target": "adaptive_lr", "relationship": "component_of", "weight": 1.0, "description": "First moment is used in numerator of update rule"},
    {"source": "second_moment", "target": "adaptive_lr", "relationship": "builds_on", "weight": 1.0, "description": "Adaptive learning rates computed from second moment"},
    {"source": "first_moment", "target": "bias_correction", "relationship": "requires", "weight": 1.0, "description": "Bias correction compensates for zero initialization"},
    {"source": "second_moment", "target": "bias_correction", "relationship": "requires", "weight": 1.0, "description": "Bias correction stabilizes variance estimates"},
    {"source": "hyperparameter_tuning", "target": "first_moment", "relationship": "optimizes", "weight": 1.0, "description": "β₁ parameter controls first moment decay rate"},
    {"source": "hyperparameter_tuning", "target": "second_moment", "relationship": "optimizes", "weight": 1.0, "description": "β₂ parameter controls second moment decay rate"},
    {"source": "adaptive_lr", "target": "adamax", "relationship": "variant_of", "weight": 1.0, "description": "AdaMax uses infinity norm instead of L2 norm"},
    {"source": "gradient_noise", "target": "adaptive_lr", "relationship": "enables", "weight": 1.0, "description": "Adaptive scaling reduces impact of gradient noise"},
    {"source": "sparse_gradients", "target": "adaptive_lr", "relationship": "enables", "weight": 1.0, "description": "Adaptive rates handle sparse gradients effectively"},
    {"source": "memory_efficiency", "target": "computational_efficiency", "relationship": "component_of", "weight": 1.0, "description": "Together enable large-scale optimization"},
    {"source": "convergence_analysis", "target": "adaptive_lr", "relationship": "validates", "weight": 1.0, "description": "Theory proves Adam's convergence properties"}
  ]
}
