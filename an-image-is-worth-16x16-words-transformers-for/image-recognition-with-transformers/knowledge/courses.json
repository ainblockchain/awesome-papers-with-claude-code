[
  {
    "id": "foundations",
    "title": "Vision Transformer Foundations",
    "description": "이미지 처리의 기초와 Vision Transformer의 핵심 입력 처리 방식을 배웁니다.",
    "concepts": [
      "convolutional_neural_networks",
      "image_patching",
      "positional_encoding",
      "cls_token"
    ],
    "lessons": [
      {
        "concept_id": "convolutional_neural_networks",
        "title": "CNN: The Foundation of Modern Image Recognition",
        "prerequisites": [],
        "key_ideas": [
          "지역 필터를 사용한 특징 추출",
          "평행이동 불변성",
          "계층적 특징 학습",
          "귀납적 편향"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "CNN의 핵심 특징인 '평행이동 불변성'이 의미하는 것은?\n1) 같은 물체가 이미지의 다른 위치에 있어도 인식할 수 있다\n2) CNN이 모든 위치에서 같은 가중치를 사용한다\n3) 위 두 개 모두\n숫자로 답하세요.",
        "explanation": "Dosovitskiy et al.(2020)의 논문은 CNN과 Vision Transformer를 비교하며 시작합니다. CNN은 1990년대부터 이미지 인식의 표준이었습니다. 왜 그럴까요?\n\nCNN의 핵심은 **지역 필터**(local filters)입니다. 작은 필터가 이미지를 슬라이딩하며 지역 패턴(예: 모서리, 질감)을 추출합니다. 이는 두 가지 귀납적 편향을 만듭니다:\n\n1. **지역성**(Locality): 인접한 픽셀들이 의미 있게 연결된다\n2. **평행이동 불변성**(Translation Invariance): 강아지가 이미지의 어느 위치에 있든 인식할 수 있다\n\n이 편향들은 **작은 데이터셋**에서 매우 효율적입니다. 하지만 Vision Transformer는 \"충분한 데이터가 있으면 이 편향이 불필요하다\"고 주장합니다.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "image_patching",
        "title": "Image Patching: Converting Images to Sequences",
        "prerequisites": [
          "convolutional_neural_networks"
        ],
        "key_ideas": [
          "이미지를 고정 크기 패치로 분할",
          "선형 임베딩",
          "시퀀스 변환",
          "NLP-스타일 처리"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "14×14 픽셀 이미지를 16×16 크기의 패치로 나누면 몇 개의 패치가 생기나요?\n1) 1개\n2) 2개\n3) 패치 크기가 이미지보다 크므로 불가능\n숫자로 답하세요.",
        "explanation": "ViT의 핵심 발명은 매우 간단합니다: **\"이미지를 단어처럼 취급하자\"**\n\n논문의 제목인 \"An Image is Worth 16x16 Words\"는 이를 직설적으로 표현합니다. ViT는:\n\n1. 이미지를 **16×16 픽셀 패치**로 분할합니다 (224×224 이미지 = 196개 패치)\n2. 각 패치를 **선형 임베딩**으로 변환합니다 (1D 벡터로)\n3. 이제 \"196개 단어\"의 시퀀스가 되어 Transformer가 처리할 수 있습니다\n\n이는 NLP의 토큰화(tokenization)와 동일합니다. CNN은 공간 구조를 보존하려고 애썼지만, ViT는 패치 시퀀스로 변환하여 **글로벌 관계**를 더 쉽게 학습합니다.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "positional_encoding",
        "title": "Positional Encoding: Teaching the Transformer 'Where' Things Are",
        "prerequisites": [
          "image_patching"
        ],
        "key_ideas": [
          "공간적 위치 정보",
          "2D 좌표 인코딩",
          "학습 가능한 임베딩",
          "위치 관계 학습"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "패치 시퀀스로 변환하면 \"패치 1이 왼쪽 상단에 있다\"는 정보가 사라집니다. 이를 복구하는 방법은?\n1) CNN을 다시 사용한다\n2) 각 패치에 위치 정보를 추가한다 (Positional Encoding)\n3) 패치의 순서로 충분하다\n숫자로 답하세요.",
        "explanation": "패치 임베딩은 \"순서 없는\" 집합입니다. [패치1, 패치2, ..., 패치196]을 [패치196, ..., 패치2, 패치1]로 섞어도 선형 임베딩 자체는 동일합니다.\n\nTransformer는 **순서를 보존하지 않습니다**. (CNN과의 주요 차이!) 따라서 ViT는 **Positional Encoding**을 추가합니다:\n\n- 각 패치 임베딩에 그것의 **위치 정보**를 더합니다\n- ViT는 이를 **학습 가능한 임베딩**으로 구현합니다 (고정된 sin/cos 함수 대신)\n- 이제 모델은 \"패치 i가 상단 왼쪽이고, 패치 j가 하단 오른쪽이다\"를 학습합니다\n\n2D 이미지 좌표를 그대로 사용하면 성능이 거의 동일하다는 점이 흥미롭습니다!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "cls_token",
        "title": "Classification Token: The Special Aggregator",
        "prerequisites": [
          "image_patching"
        ],
        "key_ideas": [
          "[CLS] 토큰",
          "BERT의 차용",
          "전역 표현 집약",
          "분류 헤드 입력"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT가 이미지 분류를 위해 196개 패치를 모두 사용하지 않고 [CLS] 토큰을 추가하는 이유는?\n1) 계산을 빠르게 하기 위해\n2) [CLS]가 모든 패치의 정보를 집약하는 \"대표 벡터\"가 되도록\n3) BERT가 이렇게 했으니까\n숫자로 답하세요.",
        "explanation": "ViT는 BERT(Natural Language Processing)의 설계를 차용했습니다. BERT는:\n\n1. 토큰 시퀀스의 **맨 앞에 특수 토큰 [CLS]를 삽입**합니다\n2. Transformer를 통과시킵니다\n3. [CLS]의 **최종 출력만** 분류에 사용합니다\n\nWhy? 통상적인 방식은 모든 패치를 평균화하는 것입니다. 하지만 [CLS] 토큰은:\n\n- Transformer 인코더에서 **모든 패치와 상호작용**합니다 (self-attention)\n- 최종 [CLS] 벡터는 **\"전체 이미지의 집약된 표현\"**이 됩니다\n- 이 하나의 벡터를 선형 분류 헤드(1000개 클래스)로 보냅니다\n\n실제로는 매우 효과적이고 elegant한 설계입니다!",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "architecture",
    "title": "Transformer Architecture",
    "description": "Vision Transformer의 핵심 신경망 아키텍처와 각 컴포넌트를 깊이 있게 학습합니다.",
    "concepts": [
      "transformer_encoder",
      "self_attention",
      "multihead_attention",
      "feedforward_network"
    ],
    "lessons": [
      {
        "concept_id": "transformer_encoder",
        "title": "Transformer Encoder: The Core Processing Engine",
        "prerequisites": [
          "positional_encoding",
          "cls_token"
        ],
        "key_ideas": [
          "멀티 헤드 어텐션 블록",
          "피드포워드 네트워크",
          "계층 정규화",
          "블록 스택"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "Transformer 인코더의 기본 구조는 여러 블록이 쌓여있습니다. 각 블록의 순서는?\n1) Self-Attention → Feed-Forward → Layer Norm\n2) Layer Norm → Self-Attention → Layer Norm → Feed-Forward\n3) Feed-Forward → Self-Attention → Layer Norm\n숫자로 답하세요.",
        "explanation": "Transformer 인코더는 \"Attention Is All You Need\"(Vaswani et al., 2017)에서 정의한 표준 설계입니다. ViT는 이를 그대로 차용합니다.\n\n각 Transformer 블록:\n```\n입력 (197개 토큰: [CLS] + 196개 패치)\n  ↓\nLayer Norm\n  ↓\nMulti-Head Self-Attention\n  ↓\n잔차 연결 (residual)\n  ↓\nLayer Norm\n  ↓\nFeed-Forward Network (2개 선형층 + GELU)\n  ↓\n잔차 연결\n  ↓\n출력\n```\n\nViT-Base는 **12개** 이런 블록을 쌓습니다. 각 단계에서:\n- **Self-Attention**: 패치들 간의 관계를 학습합니다\n- **Feed-Forward**: 각 토큰을 독립적으로 변환합니다 (비선형성 추가)\n\n표준 설계이지만, 컴퓨터 비전에 적용한 것이 혁신입니다!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "self_attention",
        "title": "Self-Attention: Learning Relationships Between Patches",
        "prerequisites": [
          "transformer_encoder"
        ],
        "key_ideas": [
          "쿼리-키-값 메커니즘",
          "가중 합계",
          "유사도 기반 가중치",
          "전역 컨텍스트"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "Self-attention에서 '어느 토큰이 중요한가'는 어떻게 결정되나요?\n1) 고정된 규칙 (예: 중앙이 항상 중요)\n2) 쿼리와 키 벡터 간의 유사도 (cosine similarity)\n3) 토큰의 순서\n숫자로 답하세요.",
        "explanation": "Self-attention은 Transformer의 심장입니다. Dosovitskiy et al.이 \"ViT는 단순히 표준 Transformer를 이미지에 적용한 것\"이라고 한 이유입니다.\n\nSelf-attention의 동작:\n\n1. 각 토큰이 **쿼리(Q), 키(K), 값(V)** 벡터로 변환됩니다\n2. **쿼리-키 유사도** 계산: Attention score = Q · K^T\n3. 소프트맥스로 정규화: 각 토큰에 대한 \"주목 가중치\" 계산\n4. **가중 합계**: 출력 = softmax(score) · V\n\n실제 사례: 이미지에서 \"개의 눈\"을 인식하려면:\n- 눈 패치의 쿼리가 \"주변 패치들(코, 입, 귀)과 관련이 있는가?\"를 체크합니다\n- 이들 패치와의 유사도가 높으면 가중치를 높입니다\n- 결과적으로 \"눈과 관련된 정보\"를 집약합니다\n\nCNN과 다르게, **전역 관계**를 직접 학습할 수 있습니다!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "multihead_attention",
        "title": "Multi-Head Attention: Multiple Perspectives at Once",
        "prerequisites": [
          "self_attention"
        ],
        "key_ideas": [
          "병렬 어텐션 헤드",
          "다양한 표현 공간",
          "헤드 결합",
          "표현력 증대"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT-Base는 12개 어텐션 헤드를 사용합니다. 왜 1개 대신 여러 개를 사용할까요?\n1) 병렬 처리로 계산을 빠르게 하기 위해\n2) 각 헤드가 다른 '관점'에서 관계를 학습하기 위해\n3) 오버피팅을 방지하기 위해\n숫자로 답하세요.",
        "explanation": "Multi-head attention은 self-attention의 강력한 확장입니다.\n\n단일 헤드 vs 멀티 헤드:\n- **1개 헤드**: 단일 관점에서만 \"어느 패치가 중요한가\"를 학습\n- **12개 헤드**: 12가지 다른 \"관점\"을 동시에 학습\n\n각 헤드가 학습할 수 있는 것:\n- **헤드 1**: 색상 관계 (\"빨간 패치들이 서로 연결되는가?\")\n- **헤드 2**: 질감 관계 (\"거친 표면끼리 모여있는가?\")\n- **헤드 3**: 객체 윤곽 (\"물체의 경계는 어디인가?\")\n- **...**\n\nViT-Base의 구성:\n- 임베딩 차원: 768\n- 헤드 수: 12\n- 각 헤드의 차원: 768 / 12 = 64\n\n최종적으로 모든 헤드의 출력을 **연결(concatenate)**하여 다시 768차원으로 만듭니다. 이렇게 하면 모델이 다양한 특징을 학습할 수 있습니다!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "feedforward_network",
        "title": "Feed-Forward Network: Local Non-Linearity",
        "prerequisites": [
          "transformer_encoder"
        ],
        "key_ideas": [
          "두 층 MLP",
          "GELU 활성화",
          "토큰별 처리",
          "비선형 변환"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "Transformer 블록의 Feed-Forward Network 구조는?\n1) 선형층 → 활성화 함수(GELU) → 선형층\n2) 선형층 → 선형층\n3) 선형층 → 선형층 → 선형층\n숫자로 답하세요.",
        "explanation": "Self-attention 다음에는 **Feed-Forward Network (FFN)**이 옵니다. 이것은 간단하지만 중요합니다.\n\nFFN의 구조:\n```\n입력: (197, 768)  [197개 토큰, 각 768차원]\n  ↓\n선형층 1: (768 → 3072)  [중간 차원으로 확장]\n  ↓\nGELU 활성화  [비선형성]\n  ↓\n선형층 2: (3072 → 768)  [원래 차원으로 축소]\n  ↓\n출력: (197, 768)\n```\n\n주요 특징:\n- **각 토큰에 독립적으로 적용됩니다** (attention과 달리 전역 상호작용 없음)\n- **비선형성**을 추가하여 모델의 표현력을 증가시킵니다\n- Self-attention만으로는 부족한 \"개별 토큰 변환\"을 담당합니다\n\nViT-Base의 경우 중간 차원은 3072 (= 768 × 4)입니다.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "theory",
    "title": "Theory & Design Choices",
    "description": "ViT의 설계 철학, 귀납적 편향, 스케일링 법칙을 이해합니다.",
    "concepts": [
      "inductive_bias",
      "scaling_laws",
      "hybrid_architecture"
    ],
    "lessons": [
      {
        "concept_id": "inductive_bias",
        "title": "Inductive Bias: The Hidden Assumptions of Models",
        "prerequisites": [
          "convolutional_neural_networks",
          "transformer_encoder"
        ],
        "key_ideas": [
          "모델의 암묵적 가정",
          "CNN의 지역성과 평행이동 불변성",
          "ViT의 최소한의 편향",
          "데이터로 학습"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT가 CNN과 가장 다른 점은?\n1) 더 많은 파라미터를 가진다\n2) 이미지를 패치로 분할한다\n3) 귀납적 편향을 최소화하고 순수하게 데이터에서 패턴을 학습한다\n숫자로 답하세요.",
        "explanation": "이것이 논문의 **철학적 핵심**입니다.\n\n**귀납적 편향(Inductive Bias)**란?\n- 모델이 학습 과정에서 \"암묵적으로 가정하는 것\"\n\nCNN의 귀납적 편향:\n1. **지역성**: 가까운 픽셀들이 의미 있게 연결됨\n2. **평행이동 불변성**: 물체가 어디에 있든 같이 인식됨\n3. **계층성**: 저수준(엣지) → 중수준(질감) → 고수준(객체)\n\n이는 작은 데이터(CIFAR-10: 60,000개 이미지)에서 **매우 효율적**입니다!\n\nViT의 철학:\n- **귀납적 편향을 최소화합니다**\n- 대신 **대규모 데이터에서 직접 학습**합니다\n- \"Larger-scale training trumps inductive bias\"\n\nDosovitskiy et al.의 실험:\n- 작은 데이터: CNN > ViT\n- 대규모 데이터(JFT-300M): ViT >> CNN\n\nThis is the key insight of the paper!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "scaling_laws",
        "title": "Scaling Laws: More Data and Compute Wins",
        "prerequisites": [
          "inductive_bias"
        ],
        "key_ideas": [
          "스케일링의 효과",
          "데이터 규모의 중요성",
          "모델 크기의 영향",
          "대규모 학습의 우월성"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT의 성능이 CNN을 능가하는 조건은?\n1) 항상\n2) ImageNet(130만 이미지) 이상의 대규모 데이터로 사전학습할 때\n3) 소규모 데이터(CIFAR-10 등)에서\n숫자로 답하세요.",
        "explanation": "Dosovitskiy et al.의 실험 결과는 매우 명확합니다:\n\n**ImageNet만으로는?**\n- ViT < ResNet\n- CNN의 귀납적 편향이 작은 데이터에서 유리합니다\n\n**JFT-300M으로 사전학습하면?**\n- ViT > ResNet (88.55% vs 84.6% on ImageNet)\n- \"귀납적 편향이 필요 없다\"\n\n**CIFAR-100?**\n- ViT: 94.55% (매우 높음)\n- CNN: 더 낮음\n\n**Why?**\n\n1. **충분한 데이터**가 있으면 모델이 \"최적의 편향\"을 **직접 학습**합니다\n2. CNN의 지역성 가정이 **오히려 제약**이 될 수 있습니다\n3. ViT의 **전역 self-attention**이 장거리 관계를 직접 캡처합니다\n\nThis is a profound insight: **with unlimited data, architectural inductive bias becomes less important than model capacity and data scale.**",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "hybrid_architecture",
        "title": "Hybrid Architecture: CNN + Transformer",
        "prerequisites": [
          "inductive_bias",
          "transformer_encoder"
        ],
        "key_ideas": [
          "CNN 스테이지",
          "Transformer 스테이지",
          "귀납적 편향 활용",
          "성능 트레이드오프"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "CNN + Transformer 하이브리드 모델의 실험 결과는?\n1) 순수 ViT와 성능이 동일하다\n2) 작은 데이터에서는 더 좋지만, 대규모 사전학습에서는 ViT만큼 좋지 않다\n3) 모든 상황에서 ViT보다 낫다\n숫자로 답하세요.",
        "explanation": "논문은 또 다른 접근을 시도합니다: **\"CNN의 귀납적 편향을 활용하되, Transformer의 글로벌 학습력도 원한다면?\"**\n\n하이브리드 아키텍처:\n```\n이미지 (224×224)\n  ↓\nCNN 스테이지 (ResNet 블록들)\n  ↓\n특징맵 축소 (14×14×768)\n  ↓\nTransformer 스테이지\n  ↓\n분류\n```\n\nCNN이 하는 일:\n- 이미지를 점진적으로 다운샘플링\n- 저수준 특징(엣지, 질감) 추출\n- \"귀납적 편향\" 활용\n\nTransformer가 하는 일:\n- CNN의 출력을 받아 고수준 특징 학습\n- 글로벌 관계 캡처\n\n**실험 결과:**\n- 작은 데이터: 하이브리드 > 순수 ViT\n- 대규모 데이터: 순수 ViT ≥ 하이브리드\n\n**결론**: \"단순성의 우아함\"을 보여줍니다. 충분한 데이터가 있으면 순수 ViT만으로 충분합니다!",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "practice",
    "title": "Training & Real-World Applications",
    "description": "대규모 사전학습, 미세 조정, 벤치마크 평가와 해석 가능성을 배웁니다.",
    "concepts": [
      "pretraining_finetuning",
      "transfer_learning",
      "imagenet_benchmark",
      "attention_visualization"
    ],
    "lessons": [
      {
        "concept_id": "pretraining_finetuning",
        "title": "Pre-training and Fine-tuning: The Two-Stage Learning",
        "prerequisites": [
          "transformer_encoder",
          "scaling_laws"
        ],
        "key_ideas": [
          "대규모 사전학습",
          "작은 데이터 미세 조정",
          "특징 추출기 학습",
          "전이 학습"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT의 높은 성능의 핵심은?\n1) 모델 크기가 커서\n2) ImageNet에서만 학습\n3) JFT-300M에서 사전학습 후 ImageNet으로 미세 조정\n숫자로 답하세요.",
        "explanation": "ViT의 성공은 **두 단계 학습**에 달려있습니다:\n\n**Stage 1: Pre-training (대규모)**\n- 데이터셋: JFT-300M (Google의 비공개 300만 이미지)\n- 작업: 자체 지도 학습 또는 이미지 분류\n- 시간: 매우 오래 (고성능 TPU 필요)\n- 결과: \"범용 특징 추출기\" 학습\n\n**Stage 2: Fine-tuning (작은 데이터)**\n- 데이터셋: ImageNet (130만 이미지)\n- 작업: 1000 클래스 분류\n- 조정: 분류 헤드만 교체 (또는 가벼운 미세 조정)\n- 시간: 몇 시간 (상대적으로 빠름)\n- 결과: 88.55% 정확도\n\n**Why this works:**\n\nJFT-300M의 \"잡다한\" 300만 이미지에서 ViT는:\n- 일반적인 패턴 인식\n- 다양한 객체·장면의 특징\n- 색상, 질감, 형태 관계\n\n를 모두 학습합니다. 이후 ImageNet으로 미세 조정하면 이미 학습된 특징을 활용하여 빠르게 수렴합니다!\n\n**현실적 조언**: 대규모 사전학습 데이터가 없으면 ViT보다 CNN을 추천합니다.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "transfer_learning",
        "title": "Transfer Learning: Standing on the Shoulders of Giants",
        "prerequisites": [
          "pretraining_finetuning"
        ],
        "key_ideas": [
          "사전학습 특징 재사용",
          "도메인 적응",
          "미세 조정 효율",
          "소규모 데이터 성능 향상"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "Transfer learning의 핵심 장점은?\n1) 모델 학습이 빨라진다\n2) 작은 데이터셋에서도 좋은 성능을 얻을 수 있다\n3) 위 두 개 모두\n숫자로 답하세요.",
        "explanation": "Transfer learning은 머신러닝의 가장 실용적인 기법입니다.\n\n**Traditional approach (transfer learning 없음):**\n```\nCIFAR-10 (60,000 이미지)\n  ↓\n처음부터 학습\n  ↓\n성능: ~95% (좋음)\n시간: 며칠\n```\n\n**Transfer learning approach (ViT):**\n```\nJFT-300M (300만 이미지)\n  ↓\n ViT 사전학습 (\"일반 특징\" 학습)\n  ↓\nCIFAR-10\n  ↓\n미세 조정 (분류 헤드만)\n  ↓\n성능: ~99% (!)\n시간: 몇 시간\n```\n\n**왜 이것이 작동할까?**\n\n1. **JFT-300M의 다양성**\n   - 동물, 풍경, 물체, 추상 패턴 등 모든 것\n   - ViT는 \"기본 시각 특징\"을 학습합니다\n\n2. **특징 재사용**\n   - \"개의 눈\"을 인식하는 능력은 CIFAR-10에서도 유용합니다\n   - 처음부터 배울 필요가 없습니다\n\n3. **미세 조정의 효율성**\n   - 분류 헤드만 학습 (매우 적은 파라미터)\n   - 백프로프 필요 없음\n\n**현실의 적용:**\n- 자신의 데이터(예: 의료 이미지)로 fine-tune\n- 훨씬 빠르고 정확한 결과\n- 이것이 ViT를 실무에서 혁명적으로 만들었습니다!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "imagenet_benchmark",
        "title": "ImageNet Benchmark: The Proving Ground",
        "prerequisites": [
          "pretraining_finetuning"
        ],
        "key_ideas": [
          "1000 클래스 벤치마크",
          "130만 이미지",
          "SOTA 비교",
          "성능 평가 지표"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT가 ImageNet에서 달성한 정확도는?\n1) 80%\n2) 85%\n3) 88.55%\n숫자로 답하세요.",
        "explanation": "ImageNet은 컴퓨터 비전의 \"올림픽\"입니다.\n\n**데이터셋:**\n- 1000개 클래스 (개, 고양이, 새, 자동차, ...)\n- 약 130만 개 이미지\n- 10년간 컴퓨터 비전의 벤치마크\n\n**역사:**\n- 2012: AlexNet (GPU 기반 CNN) - 85.2% top-1\n- 2014: VGGNet - 88.2%\n- 2015: ResNet - 92.3%\n- 2019: EfficientNet - 94%\n- 2020: ViT (with ImageNet-21k pre-training) - 88.55% (!!)\n\n**잠깐, ViT의 성능이 낮아 보이는데?**\n\n아닙니다! 더 정확히:\n- **ViT-B (Base)**: 77.9% (ImageNet-only)\n- **ViT-B + ImageNet-21k pre-training**: 85.0%\n- **ViT-L (Large) + JFT-300M pre-training**: **88.55%**\n- **ViT-L + JFT-300M + ImageNet-21k pre-training**: **89.16%**\n\n핵심: **사전학습 데이터의 규모가 중요합니다!**\n\n**논문의 주요 발견:**\n- 같은 계산량이면: ViT ≥ ResNet\n- 대규모 사전학습 후: ViT >> 기타 CNN\n- ImageNet만으로는: CNN이 더 나음\n\nThis validated Transformers as a viable (even superior) alternative to CNNs for vision!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "attention_visualization",
        "title": "Attention Visualization: Understanding What ViT Learns",
        "prerequisites": [
          "multihead_attention"
        ],
        "key_ideas": [
          "Attention 맵 시각화",
          "모델 해석 가능성",
          "패치 관계 이해",
          "전역 구조 인식"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT의 attention을 시각화했을 때, CNN과 가장 다른 점은?\n1) ViT는 주로 가운데 패치에 주목한다\n2) ViT는 초기 레이어부터 전역 구조를 인식한다 (CNN은 저수준 특징만)\n3) 거의 차이가 없다\n숫자로 답하세요.",
        "explanation": "Attention visualization은 신경망을 \"열어보는\" 아름다운 방법입니다.\n\n**시각화 방법:**\n1. 이미지를 입력합니다\n2. 각 레이어의 attention 가중치를 추출합니다\n3. 어느 패치들이 \"주목\"되는지 히트맵으로 그립니다\n4. 결과를 원본 이미지에 겹쳐 봅니다\n\n**CNN의 attention (비유):**\n- Layer 1: 엣지 (국소적)\n- Layer 2: 질감 (약간 넓음)\n- Layer 3: 객체 부분 (더 넓음)\n- ...\n- 최상위: 객체 (전역)\n\n**ViT의 attention:**\n- Layer 1: **이미 전역적!** 멀리 떨어진 패치들과의 관계 학습\n- Layer 2~12: 점점 정교해짐\n- 최상위: 의미 있는 고수준 구조\n\n**흥미로운 발견:**\n- [CLS] 토큰은 초기 레이어부터 모든 패치와 상호작용합니다\n- 후기 레이어에서는 특정 의미 있는 \"객체 부분\"에만 주목합니다\n- CNN처럼 \"점진적으로 커지는\" 수용장(receptive field) 없이도 작동합니다!\n\n**의의:**\n- 모델이 \"무엇을\" 하고 있는지 이해 가능\n- 실패 사례 분석 가능\n- Transformer의 \"블랙박스\" 상태를 조금 더 투명하게 만듭니다",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "frontier",
    "title": "Beyond Image Classification",
    "description": "Vision Transformer의 확장 가능성과 미래 연구 방향을 탐험합니다.",
    "concepts": [
      "vision_tasks"
    ],
    "lessons": [
      {
        "concept_id": "vision_tasks",
        "title": "Vision Tasks Beyond Classification: The Future",
        "prerequisites": [
          "imagenet_benchmark"
        ],
        "key_ideas": [
          "객체 탐지",
          "의미 분할",
          "인스턴스 분할",
          "멀티태스크 학습"
        ],
        "code_ref": "",
        "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words",
        "exercise": "ViT 논문은 다음으로 연구해야 할 과제로 어떤 것을 제시하나요?\n1) 더 깊은 ViT 모델 (레이어 추가)\n2) 객체 탐지, 의미 분할 등 다른 비전 작업으로의 확장\n3) 음성 인식으로의 확장\n숫자로 답하세요.",
        "explanation": "Dosovitskiy et al.의 논문은 \"분류만 잘하면 된다\"는 태도를 취하지 않습니다.\n\n**분류 (Classification):**\n- 입력: 이미지\n- 출력: 클래스 라벨 (1개)\n- 예: \"개\" (88.55% 정확도)\n\n**넘어가야 할 과제들:**\n\n1. **객체 탐지 (Object Detection)**\n   - 출력: 여러 객체의 위치 + 클래스\n   - 예: \"개 (bbox 좌표), 고양이 (bbox 좌표)\"\n   - ViT를 어떻게 적용할까? → 이후 DETR, ViLDet 등이 해결\n\n2. **의미 분할 (Semantic Segmentation)**\n   - 출력: 픽셀 단위 클래스\n   - 예: 각 픽셀이 \"개\", \"잔디\", \"하늘\"인지\n   - CNN은 이미 FCN 등으로 해결했지만, ViT는?\n\n3. **인스턴스 분할 (Instance Segmentation)**\n   - \"개체 1\", \"개체 2\" 등 개별 객체 분리\n   - 복잡도 ↑↑\n\n4. **3D 비전**\n   - 비디오 이해\n   - 3D 재구성\n\n**논문의 결론:**\n\"Transformer의 우월성은 분류에만 한정되지 않을 것이다. 다른 비전 작업으로의 확장은 흥미로운 미래 연구 주제다.\"\n\n**실제로 일어난 일:**\n- 2021: DETR (Transformer + 객체 탐지) ✓\n- 2021: Swin Transformer (계층적 ViT) ✓\n- 2022: SETR (ViT + 의미 분할) ✓\n- ...\n\nViT는 \"시작\"일 뿐이었습니다!",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  }
]