{
  "nodes": [
    {"id": "image_patching", "name": "Image Patching", "type": "component", "level": "foundational", "description": "이미지를 고정 크기의 패치(예: 16x16 픽셀)로 분할하고 선형 임베딩으로 변환하는 과정. Vision Transformer의 첫 번째 단계로, 이미지를 시퀀스 형태로 변환하여 Transformer 모델이 처리할 수 있게 만든다.", "key_ideas": ["패치 분할", "선형 임베딩", "시퀀스 변환"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "positional_encoding", "name": "Positional Encoding", "type": "component", "level": "foundational", "description": "각 패치의 공간적 위치 정보를 인코딩하여 모델에 알려주는 방법. Transformer는 순서를 명시적으로 처리하지 않으므로, 패치의 2D 위치 정보를 추가하여 공간 관계를 학습할 수 있게 한다.", "key_ideas": ["위치 정보", "2D 좌표", "학습 가능한 임베딩"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "cls_token", "name": "Classification Token", "type": "component", "level": "foundational", "description": "BERT에서 차용한 설계로, 학습 가능한 [CLS] 토큰을 패치 임베딩 시퀀스의 앞에 추가한다. Transformer 인코더를 통과한 후 이 토큰의 출력만 분류 헤드로 보내어 이미지 분류를 수행한다.", "key_ideas": ["[CLS] 토큰", "BERT 설계", "출력 토큰"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "transformer_encoder", "name": "Transformer Encoder", "type": "architecture", "level": "intermediate", "description": "다층의 Multi-Head Attention과 Feed-Forward Network로 구성된 Transformer 블록의 스택. ViT는 NLP 영역에서 검증된 표준 Transformer 인코더를 그대로 사용하여 이미지 시퀀스를 처리한다.", "key_ideas": ["멀티 헤드 어텐션", "피드포워드 네트워크", "계층 정규화"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "self_attention", "name": "Self-Attention", "type": "technique", "level": "intermediate", "description": "각 패치가 모든 다른 패치와의 관계를 학습하는 메커니즘. Self-attention은 어느 패치가 중요한지, 어떤 패치들이 함께 나타나는지를 학습하여 이미지의 의미 있는 특징을 추출한다.", "key_ideas": ["쿼리-키-값", "가중치 합", "전역 관계"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "multihead_attention", "name": "Multi-Head Attention", "type": "technique", "level": "intermediate", "description": "Attention을 여러 '헤드'로 분할하여 병렬로 처리하는 방식. 각 헤드는 서로 다른 표현 공간에서 다양한 관계(예: 색상, 질감, 객체)를 학습하여 모델의 표현력을 증가시킨다.", "key_ideas": ["병렬 처리", "다중 표현", "헤드 결합"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "feedforward_network", "name": "Feed-Forward Network", "type": "component", "level": "intermediate", "description": "Transformer 블록에서 self-attention 다음에 오는 두 개의 선형층과 활성화 함수(보통 GELU)로 구성된 신경망. 각 토큰에 독립적으로 적용되며 비선형성을 추가한다.", "key_ideas": ["선형층", "활성화 함수", "토큰별 처리"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "inductive_bias", "name": "Inductive Bias", "type": "theory", "level": "advanced", "description": "모델이 학습 과정에서 가정하는 암묵적인 편향. CNN은 지역성(locality)과 평행이동 불변성(translation invariance)을 귀납적 편향으로 가지지만, ViT는 이러한 편향을 최소화하고 순전히 데이터에서 패턴을 학습한다.", "key_ideas": ["모델 가정", "CNN의 지역성", "귀납적 편향 제거"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "pretraining_finetuning", "name": "Pre-training and Fine-tuning", "type": "training", "level": "intermediate", "description": "대규모 데이터셋(JFT-300M 등)에서 먼저 ViT를 사전학습한 후, 특정 작업(ImageNet, CIFAR-100 등)의 작은 데이터셋으로 미세 조정(fine-tuning)하는 두 단계 학습 전략. 이 방식이 ViT의 높은 성능의 핵심이다.", "key_ideas": ["대규모 사전학습", "작은 데이터셋 미세 조정", "전이 학습"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "scaling_laws", "name": "Scaling Laws", "type": "theory", "level": "advanced", "description": "모델과 데이터 규모를 증가시킬수록 성능이 향상되는 현상. ViT는 충분한 크기의 학습 데이터가 있으면 CNN 대비 상당히 더 나은 성능을 보여주며, 이는 '큰 규모 학습이 귀납적 편향을 압도한다'는 원칙을 입증한다.", "key_ideas": ["모델 크기", "데이터 크기", "성능 향상"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "hybrid_architecture", "name": "Hybrid Architecture", "type": "variant", "level": "advanced", "description": "CNN과 Transformer를 결합한 모델. 초기 CNN 스테이지에서 이미지를 다운샘플링한 후, Transformer로 처리하는 방식. 소규모 데이터셋에서 CNN의 귀납적 편향을 활용하려는 시도이지만, 대규모 데이터에서는 순수 ViT만큼 효과적이지 않다.", "key_ideas": ["CNN 스테이지", "Transformer 스테이지", "귀납적 편향 활용"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "imagenet_benchmark", "name": "ImageNet Benchmark", "type": "application", "level": "foundational", "description": "1000개 클래스의 약 130만 이미지로 구성된 대규모 이미지 분류 벤치마크. ViT는 이전 SOTA(ResNet) 대비 더 높은 정확도(88.55%)를 달성하여 Transformer의 컴퓨터 비전 적용 가능성을 증명했다.", "key_ideas": ["분류 벤치마크", "1000 클래스", "성능 평가"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "transfer_learning", "name": "Transfer Learning", "type": "technique", "level": "intermediate", "description": "한 도메인에서 학습한 지식을 다른 도메인에 적용하는 학습 전략. ViT는 큰 데이터셋(JFT-300M)에서 학습한 특징 추출기를 작은 벤치마크 작업으로 전이시켜 우수한 성능을 달성한다.", "key_ideas": ["지식 재사용", "도메인 적응", "미세 조정"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "attention_visualization", "name": "Attention Visualization", "type": "application", "level": "advanced", "description": "ViT의 attention 맵을 시각화하여 모델이 이미지의 어느 부분을 보고 있는지 이해하는 방법. 흥미롭게도, 저수준 vision CNN과 달리 ViT는 초기 레이어부터 전역 구조(global structure)에 주목한다.", "key_ideas": ["Attention map", "해석 가능성", "모델 이해"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "convolutional_neural_networks", "name": "Convolutional Neural Networks (CNNs)", "type": "architecture", "level": "foundational", "description": "지역 필터를 사용하여 이미지의 지역 패턴을 추출하는 신경망. 평행이동 불변성과 지역성의 귀납적 편향을 가져 소규모 데이터셋에서 효율적이지만, 전역 정보를 직접 캡처하기 어렵다. ViT의 비교 기준이다.", "key_ideas": ["합성곱", "지역성", "평행이동 불변성"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0},
    {"id": "vision_tasks", "name": "Vision Tasks Beyond Classification", "type": "application", "level": "frontier", "description": "이미지 분류 외에 객체 탐지(object detection), 의미 분할(semantic segmentation), 인스턴스 분할(instance segmentation) 등 다른 컴퓨터 비전 작업. ViT 논문은 이러한 확장 가능성을 제시하며 후속 연구의 방향을 제안한다.", "key_ideas": ["객체 탐지", "의미 분할", "멀티태스크 학습"], "code_refs": [], "paper_ref": "Dosovitskiy et al., 2020 — An Image is Worth 16x16 Words", "first_appeared": null, "confidence": 1.0}
  ],
  "edges": [
    {"source": "image_patching", "target": "positional_encoding", "relationship": "requires", "weight": 1.0, "description": "이미지를 패치로 나눈 후 위치 정보를 추가해야 한다"},
    {"source": "positional_encoding", "target": "transformer_encoder", "relationship": "requires", "weight": 1.0, "description": "패치와 위치 정보가 Transformer 입력이 된다"},
    {"source": "image_patching", "target": "cls_token", "relationship": "requires", "weight": 1.0, "description": "[CLS] 토큰은 패치 임베딩과 함께 처리된다"},
    {"source": "cls_token", "target": "transformer_encoder", "relationship": "requires", "weight": 1.0, "description": "[CLS] 토큰이 Transformer 입력에 포함된다"},
    {"source": "transformer_encoder", "target": "self_attention", "relationship": "component_of", "weight": 1.0, "description": "Self-attention은 Transformer의 핵심 메커니즘"},
    {"source": "self_attention", "target": "multihead_attention", "relationship": "variant_of", "weight": 1.0, "description": "Multi-head attention은 self-attention의 확장 버전"},
    {"source": "transformer_encoder", "target": "feedforward_network", "relationship": "component_of", "weight": 1.0, "description": "피드포워드 네트워크는 Transformer 블록의 일부"},
    {"source": "convolutional_neural_networks", "target": "inductive_bias", "relationship": "requires", "weight": 1.0, "description": "CNN은 지역성의 귀납적 편향을 기반으로 한다"},
    {"source": "inductive_bias", "target": "scaling_laws", "relationship": "requires", "weight": 1.0, "description": "충분한 데이터가 있으면 귀납적 편향이 덜 중요해진다"},
    {"source": "image_patching", "target": "pretraining_finetuning", "relationship": "enables", "weight": 1.0, "description": "패치 기반 설계가 대규모 사전학습을 가능하게 한다"},
    {"source": "transformer_encoder", "target": "pretraining_finetuning", "relationship": "enables", "weight": 1.0, "description": "Transformer는 대규모 사전학습에 효과적이다"},
    {"source": "pretraining_finetuning", "target": "transfer_learning", "relationship": "implements", "weight": 1.0, "description": "사전학습과 미세 조정이 전이 학습의 구체적 구현"},
    {"source": "transfer_learning", "target": "imagenet_benchmark", "relationship": "enables", "weight": 1.0, "description": "전이 학습으로 ImageNet 벤치마크에서 우수한 성능 달성"},
    {"source": "hybrid_architecture", "target": "pretraining_finetuning", "relationship": "alternative_to", "weight": 1.0, "description": "하이브리드 아키텍처는 순수 ViT의 대안이지만 덜 효과적"},
    {"source": "transformer_encoder", "target": "attention_visualization", "relationship": "enables", "weight": 1.0, "description": "Attention 맵의 시각화가 Transformer의 동작을 보여준다"},
    {"source": "imagenet_benchmark", "target": "vision_tasks", "relationship": "enables", "weight": 1.0, "description": "ImageNet에서의 성공이 다른 비전 작업으로의 확장을 가능하게 한다"}
  ]
}
