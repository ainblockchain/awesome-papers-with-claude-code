{"nodes":[{"id":"sequence_to_sequence","name":"Sequence-to-Sequence Learning","type":"architecture","level":"foundational","description":"A framework for mapping input sequences to output sequences using encoder-decoder models.","key_ideas":["encoder compresses input to context vector","decoder generates output token by token"],"code_refs":[],"paper_ref":"Sutskever et al., 2014 — Sequence to Sequence Learning with Neural Networks","first_appeared":null,"confidence":1.0},{"id":"attention_mechanism","name":"Attention Mechanism","type":"technique","level":"foundational","description":"Allows decoders to focus on relevant encoder states rather than a single context vector.","key_ideas":["alignment scores between query and keys","weighted sum of values"],"code_refs":[],"paper_ref":"Bahdanau et al., 2015 — Neural Machine Translation by Jointly Learning to Align and Translate","first_appeared":null,"confidence":1.0},{"id":"scaled_dot_product_attention","name":"Scaled Dot-Product Attention","type":"technique","level":"intermediate","description":"Computes attention as softmax(QKᵀ/√d_k)V, scaling to prevent vanishing gradients.","key_ideas":["queries, keys, values abstraction","√d_k scaling factor"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"multi_head_attention","name":"Multi-Head Attention","type":"component","level":"intermediate","description":"Runs h parallel attention heads to capture diverse relationship patterns simultaneously.","key_ideas":["8 parallel heads in base model","concatenate and project outputs"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"positional_encoding","name":"Positional Encoding","type":"technique","level":"intermediate","description":"Injects sequence order via sine/cosine functions since attention is permutation-invariant.","key_ideas":["sine/cosine at varying frequencies","added to input embeddings"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"transformer_architecture","name":"Transformer Architecture","type":"architecture","level":"intermediate","description":"A purely attention-based encoder-decoder model that eliminates recurrence and convolutions.","key_ideas":["6 encoder + 6 decoder layers","residual connections + layer norm"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"feed_forward_sublayer","name":"Position-wise Feed-Forward Network","type":"component","level":"advanced","description":"Two linear transformations with ReLU applied identically to each position.","key_ideas":["inner dimension 2048","applied position-by-position"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"encoder_decoder_stacks","name":"Encoder-Decoder Stacks","type":"architecture","level":"advanced","description":"Stacked identical layers with residual connections and layer normalization for stable deep learning.","key_ideas":["LayerNorm(x + Sublayer(x))","cross-attention in decoder"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"training_regime","name":"Training Regime","type":"training","level":"advanced","description":"Adam optimizer with warmup schedule, dropout, and label smoothing for robust Transformer training.","key_ideas":["4000-step warmup then decay","label smoothing ε=0.1"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0},{"id":"transformer_applications","name":"Transformer Applications & Impact","type":"application","level":"frontier","description":"Transformers generalize beyond MT to parsing, vision, and foundation models like BERT and GPT.","key_ideas":["BERT, GPT built on Transformer","vision and multimodal extensions"],"code_refs":[],"paper_ref":"Vaswani et al., 2017 — Attention Is All You Need","first_appeared":null,"confidence":1.0}],"edges":[{"source":"attention_mechanism","target":"scaled_dot_product_attention","relationship":"builds_on","weight":1.0,"description":"Scaled dot-product attention is a specific, efficient formulation of the general attention mechanism."},{"source":"sequence_to_sequence","target":"transformer_architecture","relationship":"evolves_to","weight":1.0,"description":"The Transformer replaces RNN-based seq2seq with a purely attention-based design."},{"source":"scaled_dot_product_attention","target":"multi_head_attention","relationship":"builds_on","weight":1.0,"description":"Multi-head attention runs scaled dot-product attention in parallel across multiple heads."},{"source":"multi_head_attention","target":"transformer_architecture","relationship":"component_of","weight":1.0,"description":"Multi-head attention is the central building block of every Transformer layer."},{"source":"positional_encoding","target":"transformer_architecture","relationship":"component_of","weight":1.0,"description":"Positional encoding compensates for the lack of recurrence in the Transformer."},{"source":"feed_forward_sublayer","target":"transformer_architecture","relationship":"component_of","weight":1.0,"description":"Each Transformer layer pairs attention with a feed-forward sublayer."},{"source":"encoder_decoder_stacks","target":"transformer_architecture","relationship":"component_of","weight":1.0,"description":"The full Transformer is realized by stacking encoder and decoder layers."},{"source":"training_regime","target":"transformer_architecture","relationship":"optimizes","weight":1.0,"description":"The warmup schedule and regularization are key to training the Transformer."},{"source":"transformer_architecture","target":"transformer_applications","relationship":"enables","weight":1.0,"description":"The general-purpose Transformer architecture enabled a wave of foundation models."}]}
