{
  "nodes": [
    {
      "id": "sequence_transduction",
      "name": "Sequence Transduction",
      "type": "application",
      "level": "foundational",
      "description": "Sequence transduction is the task of converting one sequence into another, such as translating English to German or converting speech to text. Prior to the Transformer, this was dominated by RNN-based encoder-decoder models. Understanding this problem context is essential to appreciating what the Transformer solves.",
      "key_ideas": ["input-output sequence mapping", "variable-length sequences", "machine translation as canonical task"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "recurrent_neural_networks",
      "name": "Recurrent Neural Networks",
      "type": "architecture",
      "level": "foundational",
      "description": "RNNs process sequences step-by-step, maintaining a hidden state that carries information from previous tokens. LSTMs and GRUs improved long-range memory but are inherently sequential, preventing parallelization. The Transformer was designed explicitly to overcome these limitations.",
      "key_ideas": ["sequential processing", "hidden state", "vanishing gradient problem", "LSTM/GRU variants"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "encoder_decoder_architecture",
      "name": "Encoder-Decoder Architecture",
      "type": "architecture",
      "level": "foundational",
      "description": "The encoder-decoder framework compresses an input sequence into a context representation, which the decoder then uses to generate the output sequence token by token. The Transformer adopts this structure but replaces recurrence with stacked attention layers in both encoder and decoder.",
      "key_ideas": ["context vector", "encoder stack", "decoder stack", "autoregressive generation"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_mechanism_basics",
      "name": "Attention Mechanism Basics",
      "type": "technique",
      "level": "foundational",
      "description": "Attention allows a model to dynamically focus on different parts of the input when producing each output token, rather than relying on a single fixed context vector. Originally introduced as an add-on to RNN-based models, the Transformer makes attention its sole computational primitive.",
      "key_ideas": ["dynamic focus", "context-dependent weighting", "alignment scores", "soft attention"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "token_embeddings",
      "name": "Token Embeddings",
      "type": "component",
      "level": "foundational",
      "description": "Token embeddings convert discrete tokens (words or subwords) into continuous vectors of dimension d_model=512. In the Transformer, the same embedding matrix is shared between encoder input, decoder input, and the pre-softmax linear projection, with weights scaled by √d_model.",
      "key_ideas": ["continuous representation", "d_model=512", "shared weight matrix", "√d_model scaling"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "queries_keys_values",
      "name": "Queries, Keys, and Values",
      "type": "theory",
      "level": "intermediate",
      "description": "Attention is formulated as a query looking up a database of key-value pairs. Each query vector is compared against all key vectors to produce weights, which are then used to blend the value vectors into an output. This abstraction cleanly separates what you're looking for (Q), what you index by (K), and what you retrieve (V).",
      "key_ideas": ["query-key similarity", "value retrieval", "linear projections", "information retrieval analogy"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "scaled_dot_product_attention",
      "name": "Scaled Dot-Product Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "The core attention operation computes Attention(Q,K,V) = softmax(QKᵀ/√d_k)V. The scaling by √d_k prevents the dot products from growing too large in high dimensions, which would push softmax into near-zero gradient regions. This single formula is the engine that drives the entire Transformer.",
      "key_ideas": ["dot product similarity", "√d_k scaling", "softmax normalization", "gradient stability"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "multi_head_attention",
      "name": "Multi-Head Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "Instead of one attention function over d_model dimensions, multi-head attention runs h=8 parallel attention heads, each with d_k=d_v=64. Outputs are concatenated and projected. This lets the model simultaneously attend to information from different representation subspaces at different positions.",
      "key_ideas": ["h=8 parallel heads", "d_k=d_v=64", "concatenation and projection", "diverse representation subspaces"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "positional_encoding",
      "name": "Positional Encoding",
      "type": "component",
      "level": "intermediate",
      "description": "Since self-attention is permutation-invariant, position information must be injected explicitly. The Transformer uses sinusoidal functions PE(pos,2i)=sin(pos/10000^(2i/d_model)) and PE(pos,2i+1)=cos(...). These encodings allow the model to learn relative position patterns and generalize to sequence lengths unseen during training.",
      "key_ideas": ["permutation invariance problem", "sinusoidal functions", "relative position learning", "additive to embeddings"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "position_wise_ffn",
      "name": "Position-wise Feed-Forward Networks",
      "type": "component",
      "level": "intermediate",
      "description": "After each attention sub-layer, a two-layer feed-forward network is applied independently to each position: FFN(x) = max(0, xW₁+b₁)W₂+b₂. With d_model=512 and inner dimension d_ff=2048, this provides non-linearity and capacity between attention layers. The same FFN weights are shared across positions but differ between layers.",
      "key_ideas": ["position-independent", "d_ff=2048 inner dimension", "ReLU activation", "per-layer weights"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_encoder",
      "name": "Transformer Encoder Stack",
      "type": "architecture",
      "level": "intermediate",
      "description": "The encoder is composed of N=6 identical layers, each containing a multi-head self-attention sub-layer followed by a position-wise FFN. Each sub-layer is wrapped with a residual connection and layer normalization: LayerNorm(x + Sublayer(x)). All encoder layers produce d_model=512 outputs.",
      "key_ideas": ["N=6 identical layers", "self-attention sublayer", "FFN sublayer", "residual + LayerNorm"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_decoder",
      "name": "Transformer Decoder Stack",
      "type": "architecture",
      "level": "intermediate",
      "description": "The decoder also has N=6 layers, but each contains three sub-layers: masked self-attention, encoder-decoder cross-attention, and position-wise FFN. The masking ensures autoregressive generation — each position can only attend to previous positions. Cross-attention lets the decoder query the encoder's output.",
      "key_ideas": ["masked self-attention", "encoder-decoder cross-attention", "three sub-layers", "autoregressive constraint"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "residual_connections_layernorm",
      "name": "Residual Connections & Layer Normalization",
      "type": "optimization",
      "level": "advanced",
      "description": "Every sub-layer in the Transformer uses residual connections (output = x + Sublayer(x)) followed by layer normalization. Residual connections prevent gradient vanishing in deep networks, while layer normalization stabilizes training by normalizing activations across the feature dimension rather than the batch dimension.",
      "key_ideas": ["skip connections", "gradient flow", "LayerNorm vs BatchNorm", "training stability"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "masking_in_decoder",
      "name": "Masking in Decoder",
      "type": "technique",
      "level": "advanced",
      "description": "The decoder applies a causal mask to self-attention, setting future positions to -∞ before softmax so they become zero after normalization. This prevents the model from 'seeing the future' during training, enforcing the autoregressive property needed for generation at inference time.",
      "key_ideas": ["causal masking", "-∞ before softmax", "autoregressive property", "teacher forcing during training"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "self_vs_cross_attention",
      "name": "Self-Attention vs Cross-Attention",
      "type": "theory",
      "level": "advanced",
      "description": "Self-attention has Q, K, V all derived from the same sequence — encoder attending to itself or decoder attending to its own past outputs. Cross-attention has Q from the decoder but K and V from the encoder output, allowing the decoder to selectively focus on relevant input positions for each output token.",
      "key_ideas": ["same-source Q/K/V in self-attention", "cross-source in cross-attention", "three uses in Transformer", "information flow"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_complexity",
      "name": "Computational Complexity of Attention",
      "type": "theory",
      "level": "advanced",
      "description": "Self-attention has O(n²·d) complexity per layer versus O(n·d²) for recurrence. For short sequences (n < d), attention is cheaper and fully parallelizable. The paper analyzes this trade-off extensively, noting the O(1) maximum path length between any two positions versus O(n) for RNNs, enabling better long-range dependency learning.",
      "key_ideas": ["O(n²·d) complexity", "O(1) path length", "parallelization advantage", "sequence length vs model dimension trade-off"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "lr_warmup_schedule",
      "name": "Learning Rate Warmup Schedule",
      "type": "training",
      "level": "advanced",
      "description": "The Transformer uses Adam with a custom learning rate schedule: lr = d_model^(-0.5) · min(step^(-0.5), step · warmup_steps^(-1.5)), with warmup_steps=4000. The rate increases linearly for the first 4000 steps then decreases proportionally to the inverse square root of step number, stabilizing early training.",
      "key_ideas": ["linear warmup phase", "inverse sqrt decay", "Adam optimizer", "warmup_steps=4000"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "label_smoothing",
      "name": "Label Smoothing Regularization",
      "type": "training",
      "level": "advanced",
      "description": "Instead of training with hard one-hot targets, label smoothing (ε_ls=0.1) distributes 0.1 probability mass uniformly across all vocabulary items. This prevents the model from becoming overconfident, hurts perplexity but improves BLEU score and model calibration.",
      "key_ideas": ["soft targets", "ε_ls=0.1", "overconfidence prevention", "BLEU vs perplexity trade-off"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transfer_learning_transformers",
      "name": "Transfer Learning with Transformers",
      "type": "application",
      "level": "frontier",
      "description": "The Transformer's architecture proved extraordinarily transferable — BERT, GPT, and T5 all descend directly from this paper. Pre-training on massive corpora then fine-tuning on downstream tasks became the dominant NLP paradigm, largely because the Transformer's attention mechanism captures rich linguistic structure.",
      "key_ideas": ["BERT encoder-only", "GPT decoder-only", "pre-training + fine-tuning", "universal text encoder"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_interpretability",
      "name": "Attention Visualization & Interpretability",
      "type": "application",
      "level": "frontier",
      "description": "Attention weights can be visualized to inspect what the model focuses on for each output token. The paper shows attention heads learning syntactic and semantic roles (e.g., coreference, subject-verb agreement). However, whether attention weights truly reflect model reasoning is an active debate in interpretability research.",
      "key_ideas": ["attention weight visualization", "head specialization", "interpretability debate", "BertViz and similar tools"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "beyond_translation",
      "name": "Transformers Beyond Translation",
      "type": "application",
      "level": "frontier",
      "description": "Originally designed for machine translation, Transformers now dominate vision (ViT), audio (Whisper), protein structure (AlphaFold2), code generation (Codex), and multimodal tasks. The self-attention mechanism's ability to model arbitrary pairwise relationships makes it a near-universal sequence processor.",
      "key_ideas": ["Vision Transformer (ViT)", "AlphaFold2", "multimodal models", "domain-agnostic architecture"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    }
  ],
  "edges": [
    {"source": "sequence_transduction", "target": "encoder_decoder_architecture", "relationship": "requires", "weight": 1.0, "description": "Sequence transduction tasks are typically solved with encoder-decoder models"},
    {"source": "recurrent_neural_networks", "target": "encoder_decoder_architecture", "relationship": "component_of", "weight": 1.0, "description": "RNNs were the primary building block of encoder-decoder models before the Transformer"},
    {"source": "attention_mechanism_basics", "target": "queries_keys_values", "relationship": "evolves_to", "weight": 1.0, "description": "The QKV formulation is the precise mathematical realization of the attention concept"},
    {"source": "encoder_decoder_architecture", "target": "transformer_encoder", "relationship": "evolves_to", "weight": 1.0, "description": "The Transformer encoder is the attention-based replacement for RNN encoders"},
    {"source": "encoder_decoder_architecture", "target": "transformer_decoder", "relationship": "evolves_to", "weight": 1.0, "description": "The Transformer decoder replaces the RNN decoder with stacked attention layers"},
    {"source": "token_embeddings", "target": "positional_encoding", "relationship": "requires", "weight": 1.0, "description": "Positional encodings are added to token embeddings to inject position information"},
    {"source": "queries_keys_values", "target": "scaled_dot_product_attention", "relationship": "component_of", "weight": 1.0, "description": "Scaled dot-product attention is the formula that operates on Q, K, V"},
    {"source": "scaled_dot_product_attention", "target": "multi_head_attention", "relationship": "component_of", "weight": 1.0, "description": "Multi-head attention runs scaled dot-product attention in parallel across h heads"},
    {"source": "multi_head_attention", "target": "transformer_encoder", "relationship": "component_of", "weight": 1.0, "description": "Multi-head self-attention is the primary sub-layer in each encoder layer"},
    {"source": "multi_head_attention", "target": "transformer_decoder", "relationship": "component_of", "weight": 1.0, "description": "Multi-head attention appears twice in the decoder: as masked self-attention and cross-attention"},
    {"source": "position_wise_ffn", "target": "transformer_encoder", "relationship": "component_of", "weight": 1.0, "description": "The FFN is the second sub-layer in each encoder layer"},
    {"source": "position_wise_ffn", "target": "transformer_decoder", "relationship": "component_of", "weight": 1.0, "description": "The FFN is the third sub-layer in each decoder layer"},
    {"source": "residual_connections_layernorm", "target": "transformer_encoder", "relationship": "component_of", "weight": 1.0, "description": "Every sub-layer in the encoder uses residual connections and layer normalization"},
    {"source": "masking_in_decoder", "target": "transformer_decoder", "relationship": "component_of", "weight": 1.0, "description": "Causal masking is applied in the decoder's self-attention to enforce autoregressive generation"},
    {"source": "self_vs_cross_attention", "target": "transformer_decoder", "relationship": "component_of", "weight": 1.0, "description": "The decoder uses both self-attention and cross-attention, making this distinction critical"},
    {"source": "recurrent_neural_networks", "target": "attention_complexity", "relationship": "alternative_to", "weight": 1.0, "description": "Attention complexity is analyzed in comparison to RNN computational complexity"},
    {"source": "lr_warmup_schedule", "target": "label_smoothing", "relationship": "enables", "weight": 0.8, "description": "Both are regularization/training techniques used together to train the Transformer"},
    {"source": "transformer_encoder", "target": "transfer_learning_transformers", "relationship": "enables", "weight": 1.0, "description": "The encoder-only variant of the Transformer became BERT, the foundation of transfer learning"},
    {"source": "transformer_decoder", "target": "transfer_learning_transformers", "relationship": "enables", "weight": 1.0, "description": "The decoder-only variant became GPT, powering generative transfer learning"},
    {"source": "multi_head_attention", "target": "attention_interpretability", "relationship": "enables", "weight": 1.0, "description": "Multi-head attention weights are the primary object of attention visualization research"},
    {"source": "transformer_encoder", "target": "beyond_translation", "relationship": "enables", "weight": 1.0, "description": "The Transformer architecture was adapted to vision, audio, proteins, and more"}
  ]
}
