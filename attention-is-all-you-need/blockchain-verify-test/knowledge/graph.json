{
  "nodes": [
    {
      "id": "sequence_to_sequence",
      "name": "Sequence-to-Sequence Models",
      "type": "architecture",
      "level": "foundational",
      "description": "Neural network architectures that transform an input sequence into an output sequence. Before Transformers, these were typically built using recurrent neural networks (RNNs) or convolutional neural networks (CNNs).",
      "key_ideas": ["Encoder-decoder paradigm", "Variable-length input/output", "Foundation for machine translation"],
      "code_refs": [],
      "paper_ref": "Sutskever et al., 2014 — Sequence to Sequence Learning with Neural Networks",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_mechanism",
      "name": "Attention Mechanism",
      "type": "technique",
      "level": "foundational",
      "description": "A mechanism that allows models to focus on relevant parts of the input when producing each part of the output. It computes weighted sums of values based on query-key similarity.",
      "key_ideas": ["Query-key-value paradigm", "Weighted aggregation", "Dynamic focus on relevant information"],
      "code_refs": [],
      "paper_ref": "Bahdanau et al., 2015 — Neural Machine Translation by Jointly Learning to Align and Translate",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "dot_product_attention",
      "name": "Dot-Product Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "A specific attention mechanism that computes attention weights using dot products between queries and keys. Efficient and parallelizable compared to additive attention.",
      "key_ideas": ["Dot product similarity", "Softmax normalization", "Matrix multiplication efficiency"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "scaled_dot_product_attention",
      "name": "Scaled Dot-Product Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "Dot-product attention with scaling by 1/√d_k to prevent gradient saturation when dimension sizes are large. The core attention operation: Attention(Q,K,V) = softmax(QK^T/√d_k)V.",
      "key_ideas": ["Scaling factor 1/√d_k", "Prevents softmax saturation", "Stable gradients for large dimensions"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "multi_head_attention",
      "name": "Multi-Head Attention",
      "type": "component",
      "level": "intermediate",
      "description": "Running multiple attention operations in parallel, each with different learned projections. Allows the model to attend to information from different representation subspaces at different positions.",
      "key_ideas": ["Parallel attention heads (h=8)", "Different learned projections", "Concatenation and linear transformation"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "self_attention",
      "name": "Self-Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "Attention where queries, keys, and values all come from the same sequence. Allows each position to attend to all positions in the same sequence, capturing dependencies regardless of distance.",
      "key_ideas": ["Same-sequence attention", "O(1) path length between positions", "Global context at every layer"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "positional_encoding",
      "name": "Positional Encoding",
      "type": "component",
      "level": "intermediate",
      "description": "Since the Transformer has no recurrence or convolution, positional encodings are added to input embeddings to inject sequence order information. Uses sinusoidal functions: PE(pos,2i) = sin(pos/10000^(2i/d_model)).",
      "key_ideas": ["Sinusoidal position signals", "Relative position learning", "No sequential bottleneck"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "encoder_stack",
      "name": "Transformer Encoder",
      "type": "component",
      "level": "advanced",
      "description": "Stack of N=6 identical layers, each containing multi-head self-attention and position-wise feed-forward network, with residual connections and layer normalization around each sub-layer.",
      "key_ideas": ["N=6 identical layers", "Self-attention + FFN per layer", "Residual connections + LayerNorm"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "decoder_stack",
      "name": "Transformer Decoder",
      "type": "component",
      "level": "advanced",
      "description": "Stack of N=6 identical layers with masked self-attention, encoder-decoder attention, and feed-forward network. Masking ensures predictions depend only on known outputs at positions less than the current position.",
      "key_ideas": ["Masked self-attention", "Encoder-decoder cross-attention", "Auto-regressive generation"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "feed_forward_network",
      "name": "Position-wise Feed-Forward Network",
      "type": "component",
      "level": "intermediate",
      "description": "Two linear transformations with ReLU activation applied to each position independently: FFN(x) = max(0, xW₁+b₁)W₂+b₂. Inner dimension is 2048, outer is 512.",
      "key_ideas": ["Two linear layers with ReLU", "Applied position-wise", "d_ff=2048, d_model=512"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "residual_connections",
      "name": "Residual Connections",
      "type": "technique",
      "level": "foundational",
      "description": "Skip connections that add the input of a sub-layer to its output, enabling gradient flow and allowing the network to learn identity mappings easily. Used around every sub-layer in the Transformer.",
      "key_ideas": ["Skip connections", "Gradient flow", "LayerNorm(x + Sublayer(x))"],
      "code_refs": [],
      "paper_ref": "He et al., 2016 — Deep Residual Learning for Image Recognition",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "layer_normalization",
      "name": "Layer Normalization",
      "type": "technique",
      "level": "foundational",
      "description": "Normalizes activations across the feature dimension for each training example independently. Applied after residual connections in the Transformer to stabilize training.",
      "key_ideas": ["Per-example normalization", "Stabilizes training", "Independent of batch size"],
      "code_refs": [],
      "paper_ref": "Ba et al., 2016 — Layer Normalization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "masked_attention",
      "name": "Masked Self-Attention",
      "type": "technique",
      "level": "advanced",
      "description": "Self-attention with masking to prevent positions from attending to subsequent positions. Ensures the auto-regressive property during training by setting attention weights to -∞ for illegal connections.",
      "key_ideas": ["Future position masking", "Auto-regressive constraint", "Training parallelization"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "encoder_decoder_attention",
      "name": "Encoder-Decoder Attention",
      "type": "component",
      "level": "advanced",
      "description": "Cross-attention layer in the decoder where queries come from the previous decoder layer, and keys/values come from the encoder output. Allows the decoder to attend to all input positions.",
      "key_ideas": ["Cross-attention between encoder/decoder", "Queries from decoder, K/V from encoder", "Global input context for generation"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_architecture",
      "name": "Transformer Architecture",
      "type": "architecture",
      "level": "advanced",
      "description": "The complete architecture combining encoder and decoder stacks, using only attention mechanisms without recurrence or convolution. First sequence transduction model based entirely on self-attention.",
      "key_ideas": ["Encoder-decoder with attention only", "No recurrence or convolution", "Highly parallelizable"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_dropout",
      "name": "Attention Dropout",
      "type": "optimization",
      "level": "intermediate",
      "description": "Dropout applied to attention weights after softmax. Prevents over-reliance on specific attention patterns and improves generalization. Rate of 0.1 used in the original Transformer.",
      "key_ideas": ["Regularization for attention", "Applied after softmax", "P_drop = 0.1"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "label_smoothing",
      "name": "Label Smoothing",
      "type": "optimization",
      "level": "advanced",
      "description": "Regularization technique that softens target distributions during training. Instead of hard 0/1 targets, uses ε_ls=0.1 smoothing. Hurts perplexity but improves BLEU scores.",
      "key_ideas": ["Soft target distributions", "ε_ls = 0.1", "Improved translation quality"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "warmup_learning_rate",
      "name": "Learning Rate Warmup",
      "type": "training",
      "level": "advanced",
      "description": "Learning rate schedule that linearly increases for warmup_steps, then decreases proportionally to the inverse square root of step number. Critical for stable Transformer training.",
      "key_ideas": ["Linear warmup phase", "Inverse sqrt decay", "warmup_steps = 4000"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "beam_search",
      "name": "Beam Search Decoding",
      "type": "technique",
      "level": "intermediate",
      "description": "Decoding strategy that maintains top-k hypotheses at each step. Used with beam size 4 and length penalty α=0.6 for the Transformer translation results.",
      "key_ideas": ["Top-k hypothesis tracking", "Length penalty normalization", "Beam size = 4"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "byte_pair_encoding",
      "name": "Byte Pair Encoding (BPE)",
      "type": "tokenization",
      "level": "foundational",
      "description": "Subword tokenization algorithm that iteratively merges frequent character pairs. Used for the Transformer's shared source-target vocabulary of ~37000 tokens.",
      "key_ideas": ["Subword units", "Iterative merging", "Shared vocabulary"],
      "code_refs": [],
      "paper_ref": "Sennrich et al., 2016 — Neural Machine Translation of Rare Words with Subword Units",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "parallelization",
      "name": "Training Parallelization",
      "type": "optimization",
      "level": "advanced",
      "description": "The Transformer's lack of sequential dependencies enables massive parallelization during training. Training completed in 3.5 days on 8 P100 GPUs vs weeks for RNN-based models.",
      "key_ideas": ["No sequential bottleneck", "GPU utilization", "12-100x faster training"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "machine_translation",
      "name": "Neural Machine Translation",
      "type": "application",
      "level": "frontier",
      "description": "The primary application demonstrating Transformer superiority. Achieved 28.4 BLEU on WMT 2014 En-De and 41.8 BLEU on En-Fr, surpassing all previous models including ensembles.",
      "key_ideas": ["State-of-the-art BLEU scores", "WMT 2014 benchmarks", "Single model beats ensembles"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "constituency_parsing",
      "name": "Constituency Parsing Transfer",
      "type": "application",
      "level": "frontier",
      "description": "Demonstrated Transformer generalization to English constituency parsing. Achieved competitive results with task-specific models, showing the architecture's versatility beyond translation.",
      "key_ideas": ["Generalization to parsing", "Limited task-specific tuning", "Architecture versatility"],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    }
  ],
  "edges": [
    {
      "source": "sequence_to_sequence",
      "target": "transformer_architecture",
      "relationship": "evolves_to",
      "weight": 1.0,
      "description": "Transformer is a new seq2seq architecture that replaces recurrence with attention"
    },
    {
      "source": "attention_mechanism",
      "target": "dot_product_attention",
      "relationship": "evolves_to",
      "weight": 1.0,
      "description": "Dot-product attention is a specific efficient implementation of attention"
    },
    {
      "source": "dot_product_attention",
      "target": "scaled_dot_product_attention",
      "relationship": "optimizes",
      "weight": 1.0,
      "description": "Scaling prevents softmax saturation for large dimensions"
    },
    {
      "source": "scaled_dot_product_attention",
      "target": "multi_head_attention",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Multi-head attention runs scaled dot-product attention in parallel"
    },
    {
      "source": "attention_mechanism",
      "target": "self_attention",
      "relationship": "variant_of",
      "weight": 1.0,
      "description": "Self-attention applies attention within the same sequence"
    },
    {
      "source": "self_attention",
      "target": "multi_head_attention",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Multi-head attention uses self-attention with multiple heads"
    },
    {
      "source": "multi_head_attention",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Encoder uses multi-head self-attention in each layer"
    },
    {
      "source": "multi_head_attention",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Decoder uses multi-head attention for both self and cross attention"
    },
    {
      "source": "feed_forward_network",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Each encoder layer contains a position-wise FFN"
    },
    {
      "source": "feed_forward_network",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Each decoder layer contains a position-wise FFN"
    },
    {
      "source": "residual_connections",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Residual connections wrap every sub-layer in encoder"
    },
    {
      "source": "residual_connections",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Residual connections wrap every sub-layer in decoder"
    },
    {
      "source": "layer_normalization",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "LayerNorm applied after residual connections in encoder"
    },
    {
      "source": "layer_normalization",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "LayerNorm applied after residual connections in decoder"
    },
    {
      "source": "positional_encoding",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Positional encodings inject sequence order into the model"
    },
    {
      "source": "self_attention",
      "target": "masked_attention",
      "relationship": "variant_of",
      "weight": 1.0,
      "description": "Masked attention is self-attention with future position masking"
    },
    {
      "source": "masked_attention",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Decoder uses masked self-attention for auto-regressive generation"
    },
    {
      "source": "multi_head_attention",
      "target": "encoder_decoder_attention",
      "relationship": "variant_of",
      "weight": 1.0,
      "description": "Encoder-decoder attention is multi-head attention with cross-sequence Q/KV"
    },
    {
      "source": "encoder_decoder_attention",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Decoder uses encoder-decoder attention to attend to input"
    },
    {
      "source": "encoder_stack",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Encoder stack is one half of the Transformer"
    },
    {
      "source": "decoder_stack",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Decoder stack is the other half of the Transformer"
    },
    {
      "source": "attention_dropout",
      "target": "multi_head_attention",
      "relationship": "optimizes",
      "weight": 0.8,
      "description": "Dropout regularizes attention weights"
    },
    {
      "source": "label_smoothing",
      "target": "transformer_architecture",
      "relationship": "optimizes",
      "weight": 0.8,
      "description": "Label smoothing improves translation quality"
    },
    {
      "source": "warmup_learning_rate",
      "target": "transformer_architecture",
      "relationship": "enables",
      "weight": 0.9,
      "description": "Learning rate warmup is critical for stable training"
    },
    {
      "source": "byte_pair_encoding",
      "target": "transformer_architecture",
      "relationship": "enables",
      "weight": 0.7,
      "description": "BPE provides the subword vocabulary for input/output"
    },
    {
      "source": "beam_search",
      "target": "machine_translation",
      "relationship": "enables",
      "weight": 0.8,
      "description": "Beam search decoding produces final translation outputs"
    },
    {
      "source": "transformer_architecture",
      "target": "parallelization",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Attention-only architecture enables massive parallelization"
    },
    {
      "source": "transformer_architecture",
      "target": "machine_translation",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Transformer achieves state-of-the-art translation results"
    },
    {
      "source": "transformer_architecture",
      "target": "constituency_parsing",
      "relationship": "enables",
      "weight": 0.9,
      "description": "Transformer generalizes to parsing tasks"
    }
  ]
}
