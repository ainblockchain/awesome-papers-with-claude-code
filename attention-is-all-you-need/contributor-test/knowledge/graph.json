{
  "nodes": [
    {
      "id": "sequence_to_sequence",
      "name": "Sequence-to-Sequence Models",
      "type": "architecture",
      "level": "foundational",
      "description": "Neural network architecture that maps input sequences to output sequences. Forms the basis for machine translation, text summarization, and other transduction tasks.",
      "key_ideas": [
        "Encoder processes input sequence into fixed representation",
        "Decoder generates output sequence from encoded representation",
        "Originally relied on RNNs or CNNs for sequence processing"
      ],
      "code_refs": [],
      "paper_ref": "Sutskever et al., 2014 — Sequence to Sequence Learning with Neural Networks",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_mechanism",
      "name": "Attention Mechanism",
      "type": "technique",
      "level": "foundational",
      "description": "Mechanism that allows models to focus on relevant parts of the input when producing each output element. Computes weighted combinations of values based on query-key compatibility.",
      "key_ideas": [
        "Queries attend to keys to retrieve relevant values",
        "Soft attention computes differentiable weighted sums",
        "Removes information bottleneck of fixed-size encodings"
      ],
      "code_refs": [],
      "paper_ref": "Bahdanau et al., 2015 — Neural Machine Translation by Jointly Learning to Align and Translate",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "recurrent_limitations",
      "name": "Limitations of Recurrent Models",
      "type": "theory",
      "level": "foundational",
      "description": "RNNs process sequences sequentially, creating computational bottlenecks and difficulty learning long-range dependencies due to vanishing gradients.",
      "key_ideas": [
        "Sequential processing prevents parallelization",
        "Long paths between distant positions hinder learning",
        "Memory constraints limit batch sizes for long sequences"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_architecture",
      "name": "Transformer Architecture",
      "type": "architecture",
      "level": "intermediate",
      "description": "Novel architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions. Enables unprecedented parallelization and achieves state-of-the-art results.",
      "key_ideas": [
        "Encoder-decoder structure with stacked layers",
        "Self-attention replaces recurrence for sequence modeling",
        "Constant number of operations between any two positions"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "scaled_dot_product_attention",
      "name": "Scaled Dot-Product Attention",
      "type": "component",
      "level": "intermediate",
      "description": "Core attention computation: Attention(Q,K,V) = softmax(QK^T/√d_k)V. Scaling by √d_k prevents softmax saturation for large dimensions.",
      "key_ideas": [
        "Dot product measures query-key similarity",
        "Scaling factor prevents gradient vanishing in softmax",
        "More efficient than additive attention for large d_k"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "multi_head_attention",
      "name": "Multi-Head Attention",
      "type": "component",
      "level": "intermediate",
      "description": "Runs multiple attention operations in parallel with different learned projections, allowing the model to attend to information from different representation subspaces.",
      "key_ideas": [
        "h=8 parallel attention heads with d_k=d_v=64",
        "Each head learns different attention patterns",
        "Outputs concatenated and projected back to model dimension"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "self_attention",
      "name": "Self-Attention",
      "type": "technique",
      "level": "intermediate",
      "description": "Attention where queries, keys, and values all come from the same sequence. Allows each position to attend to all positions in the previous layer.",
      "key_ideas": [
        "Q, K, V derived from the same input sequence",
        "Captures dependencies regardless of distance",
        "O(1) sequential operations vs O(n) for RNNs"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "positional_encoding",
      "name": "Positional Encoding",
      "type": "component",
      "level": "intermediate",
      "description": "Since the Transformer lacks recurrence, positional information is injected via sinusoidal functions of different frequencies added to input embeddings.",
      "key_ideas": [
        "sin/cos functions at different frequencies encode position",
        "Allows model to extrapolate to longer sequences",
        "PE(pos,2i) = sin(pos/10000^(2i/d_model))"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "encoder_stack",
      "name": "Encoder Stack",
      "type": "component",
      "level": "intermediate",
      "description": "Stack of N=6 identical layers, each containing multi-head self-attention and position-wise feed-forward network, with residual connections and layer normalization.",
      "key_ideas": [
        "Each layer has two sub-layers with residual connections",
        "Layer normalization applied after each sub-layer",
        "All layers produce outputs of dimension d_model=512"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "decoder_stack",
      "name": "Decoder Stack",
      "type": "component",
      "level": "intermediate",
      "description": "Stack of N=6 identical layers similar to encoder, but with masked self-attention and additional cross-attention over encoder outputs.",
      "key_ideas": [
        "Masked self-attention prevents attending to future positions",
        "Cross-attention attends to encoder output",
        "Generates output autoregressively, one token at a time"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "position_wise_ffn",
      "name": "Position-wise Feed-Forward Network",
      "type": "component",
      "level": "intermediate",
      "description": "Two-layer fully connected network applied identically to each position: FFN(x) = max(0, xW₁+b₁)W₂+b₂ with inner dimension 2048.",
      "key_ideas": [
        "Applied to each position separately and identically",
        "Inner layer dimension 4x the model dimension",
        "ReLU activation between the two linear transformations"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "residual_connections",
      "name": "Residual Connections",
      "type": "technique",
      "level": "intermediate",
      "description": "Skip connections that add the input of a sub-layer to its output, enabling gradient flow and allowing the network to learn identity mappings easily.",
      "key_ideas": [
        "Output = LayerNorm(x + Sublayer(x))",
        "Facilitates training of deep networks",
        "Requires all sub-layers to have same output dimension"
      ],
      "code_refs": [],
      "paper_ref": "He et al., 2016 — Deep Residual Learning for Image Recognition",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "layer_normalization",
      "name": "Layer Normalization",
      "type": "technique",
      "level": "intermediate",
      "description": "Normalization technique that normalizes across features for each training example, stabilizing training and reducing sensitivity to initialization.",
      "key_ideas": [
        "Normalizes across the feature dimension, not batch",
        "Applied after each sub-layer with residual",
        "Helps with training stability in deep networks"
      ],
      "code_refs": [],
      "paper_ref": "Ba et al., 2016 — Layer Normalization",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "attention_masking",
      "name": "Attention Masking",
      "type": "technique",
      "level": "advanced",
      "description": "Masking future positions in decoder self-attention by setting attention weights to -∞ before softmax, ensuring autoregressive property.",
      "key_ideas": [
        "Prevents decoder from attending to future tokens",
        "Implemented by adding -∞ to illegal connections",
        "Preserves autoregressive generation during training"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "cross_attention",
      "name": "Cross-Attention (Encoder-Decoder Attention)",
      "type": "technique",
      "level": "advanced",
      "description": "Attention where queries come from the decoder and keys/values come from the encoder output, allowing the decoder to attend to all input positions.",
      "key_ideas": [
        "Queries from decoder, keys and values from encoder",
        "Every decoder position can attend to all encoder positions",
        "Enables conditioning on the full input sequence"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "warmup_learning_rate",
      "name": "Learning Rate Warmup",
      "type": "optimization",
      "level": "advanced",
      "description": "Training strategy that linearly increases learning rate for the first warmup_steps, then decreases proportionally to the inverse square root of the step number.",
      "key_ideas": [
        "lrate = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))",
        "4000 warmup steps in the paper",
        "Prevents early training instability"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "label_smoothing",
      "name": "Label Smoothing",
      "type": "optimization",
      "level": "advanced",
      "description": "Regularization technique that softens target distributions by distributing probability mass from the true label to other labels, improving generalization.",
      "key_ideas": [
        "ε=0.1 probability mass distributed to other labels",
        "Hurts perplexity but improves BLEU scores",
        "Prevents overconfident predictions"
      ],
      "code_refs": [],
      "paper_ref": "Szegedy et al., 2016 — Rethinking the Inception Architecture for Computer Vision",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "dropout_regularization",
      "name": "Dropout in Transformers",
      "type": "optimization",
      "level": "advanced",
      "description": "Applied to the output of each sub-layer before residual addition, to attention weights, and to embeddings. Rate of 0.1 for base model.",
      "key_ideas": [
        "Applied to sublayer outputs, attention weights, embeddings",
        "P_drop = 0.1 for base, 0.3 for big model",
        "Critical for preventing overfitting"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_training",
      "name": "Transformer Training Setup",
      "type": "training",
      "level": "advanced",
      "description": "Training configuration including Adam optimizer with β1=0.9, β2=0.98, batch sizes of ~25000 tokens, and 8 P100 GPUs for 12 hours (base) or 3.5 days (big).",
      "key_ideas": [
        "Adam optimizer with custom betas",
        "Batch size of ~25000 source and target tokens",
        "Base model: 100K steps, Big model: 300K steps"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "computational_complexity",
      "name": "Computational Complexity Analysis",
      "type": "theory",
      "level": "advanced",
      "description": "Self-attention has O(n²·d) complexity per layer but O(1) sequential operations, compared to RNN's O(n·d²) and O(n) sequential operations.",
      "key_ideas": [
        "Self-attention: O(n²·d) complexity, O(1) sequential",
        "RNN: O(n·d²) complexity, O(n) sequential",
        "Favorable when sequence length n < dimension d"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "beam_search",
      "name": "Beam Search Decoding",
      "type": "technique",
      "level": "advanced",
      "description": "Inference strategy that maintains top-k hypotheses at each step. The paper uses beam size 4 with length penalty α=0.6 for translation.",
      "key_ideas": [
        "Keeps k best partial hypotheses at each step",
        "Length penalty prevents preference for short outputs",
        "Beam size 4 used in the paper"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "transformer_variants",
      "name": "Transformer Variants and Ablations",
      "type": "theory",
      "level": "advanced",
      "description": "The paper explores variations including different numbers of heads, key/value dimensions, model sizes, and replacement of attention with learned positional embeddings.",
      "key_ideas": [
        "Single-head attention hurts quality significantly",
        "Too many heads (h=32) also degrades quality",
        "Learned positional embeddings perform similarly to sinusoidal"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "machine_translation_results",
      "name": "Machine Translation Results",
      "type": "application",
      "level": "frontier",
      "description": "The Transformer achieved 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French, establishing new state-of-the-art with fraction of training cost.",
      "key_ideas": [
        "EN-DE: 28.4 BLEU, 2+ points over previous best",
        "EN-FR: 41.8 BLEU with 1/4 the training cost",
        "Big model trained in 3.5 days on 8 GPUs"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "parsing_results",
      "name": "English Constituency Parsing",
      "type": "application",
      "level": "frontier",
      "description": "Transformer generalized well to parsing, achieving 92.7 F1 with semi-supervised training, demonstrating the architecture's broader applicability.",
      "key_ideas": [
        "Trained on WSJ portion of Penn Treebank",
        "92.7 F1 with semi-supervised setting",
        "Competitive despite task-specific tuning in baselines"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    },
    {
      "id": "future_directions",
      "name": "Future Directions",
      "type": "theory",
      "level": "frontier",
      "description": "The paper outlines future work including applying Transformers to images, audio, video, and making generation less sequential through local attention.",
      "key_ideas": [
        "Extend to image, audio, and video modalities",
        "Investigate local, restricted attention for efficiency",
        "Make generation less sequential"
      ],
      "code_refs": [],
      "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
      "first_appeared": null,
      "confidence": 1.0
    }
  ],
  "edges": [
    {
      "source": "sequence_to_sequence",
      "target": "transformer_architecture",
      "relationship": "evolves_to",
      "weight": 1.0,
      "description": "Transformer replaces RNN-based seq2seq with attention-only architecture"
    },
    {
      "source": "attention_mechanism",
      "target": "scaled_dot_product_attention",
      "relationship": "evolves_to",
      "weight": 1.0,
      "description": "Scaled dot-product attention is the specific attention formulation used in Transformers"
    },
    {
      "source": "recurrent_limitations",
      "target": "transformer_architecture",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Limitations of RNNs motivated the development of attention-only Transformers"
    },
    {
      "source": "scaled_dot_product_attention",
      "target": "multi_head_attention",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Multi-head attention runs multiple scaled dot-product attention in parallel"
    },
    {
      "source": "multi_head_attention",
      "target": "self_attention",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Multi-head attention is the mechanism that implements self-attention"
    },
    {
      "source": "self_attention",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Self-attention is the core component of each encoder layer"
    },
    {
      "source": "self_attention",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Masked self-attention is used in each decoder layer"
    },
    {
      "source": "positional_encoding",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Positional encoding provides position information without recurrence"
    },
    {
      "source": "encoder_stack",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Encoder stack is one half of the full Transformer architecture"
    },
    {
      "source": "decoder_stack",
      "target": "transformer_architecture",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Decoder stack is the other half of the full Transformer architecture"
    },
    {
      "source": "position_wise_ffn",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "FFN is applied after self-attention in each encoder layer"
    },
    {
      "source": "position_wise_ffn",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "FFN is applied after attention sublayers in each decoder layer"
    },
    {
      "source": "residual_connections",
      "target": "encoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Residual connections wrap each sublayer in the encoder"
    },
    {
      "source": "residual_connections",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Residual connections wrap each sublayer in the decoder"
    },
    {
      "source": "layer_normalization",
      "target": "residual_connections",
      "relationship": "requires",
      "weight": 1.0,
      "description": "Layer normalization is applied with residual connections"
    },
    {
      "source": "attention_masking",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Masking ensures autoregressive property in decoder self-attention"
    },
    {
      "source": "cross_attention",
      "target": "decoder_stack",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Cross-attention allows decoder to attend to encoder outputs"
    },
    {
      "source": "multi_head_attention",
      "target": "cross_attention",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Cross-attention uses multi-head attention mechanism"
    },
    {
      "source": "warmup_learning_rate",
      "target": "transformer_training",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Learning rate warmup is critical for Transformer training stability"
    },
    {
      "source": "label_smoothing",
      "target": "transformer_training",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Label smoothing improves generalization in Transformer training"
    },
    {
      "source": "dropout_regularization",
      "target": "transformer_training",
      "relationship": "component_of",
      "weight": 1.0,
      "description": "Dropout prevents overfitting during Transformer training"
    },
    {
      "source": "transformer_architecture",
      "target": "transformer_training",
      "relationship": "requires",
      "weight": 1.0,
      "description": "Architecture requires specific training setup for best results"
    },
    {
      "source": "computational_complexity",
      "target": "transformer_architecture",
      "relationship": "optimizes",
      "weight": 1.0,
      "description": "Favorable complexity enables efficient training"
    },
    {
      "source": "beam_search",
      "target": "decoder_stack",
      "relationship": "requires",
      "weight": 1.0,
      "description": "Beam search is used for inference with the decoder"
    },
    {
      "source": "transformer_architecture",
      "target": "transformer_variants",
      "relationship": "evolves_to",
      "weight": 1.0,
      "description": "Base architecture spawns various ablations and variants"
    },
    {
      "source": "transformer_training",
      "target": "machine_translation_results",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Proper training setup leads to state-of-the-art results"
    },
    {
      "source": "transformer_architecture",
      "target": "parsing_results",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Architecture generalizes well to parsing task"
    },
    {
      "source": "transformer_architecture",
      "target": "future_directions",
      "relationship": "enables",
      "weight": 1.0,
      "description": "Success opens avenues for future research"
    }
  ]
}
