{
  "title": "Attention Is All You Need",
  "description": "The Transformer architecture replaces recurrence entirely with self-attention, achieving state-of-the-art results on machine translation while enabling full training parallelization.",
  "arxivId": "1706.03762",
  "githubUrl": "https://github.com/tensorflow/tensor2tensor",
  "authors": [
    { "name": "Ashish Vaswani" },
    { "name": "Noam Shazeer" },
    { "name": "Niki Parmar" },
    { "name": "Jakob Uszkoreit" },
    { "name": "Llion Jones" },
    { "name": "Aidan N. Gomez" },
    { "name": "≈Åukasz Kaiser" },
    { "name": "Illia Polosukhin" }
  ],
  "publishedAt": "2017-06-12",
  "organization": { "name": "Google Brain" },
  "submittedBy": "community"
}
