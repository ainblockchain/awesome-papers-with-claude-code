# BERT: Pre-training of Deep Bidirectional Transformers Learning Path

A Claude Code-powered interactive learning path based on
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018.

## Getting Started

1. Open Claude Code in this directory:
   cd bert-pre-training-of-deep-bidirectional/bert-fundamentals/
   claude
2. Start learning â€” just chat naturally:
   explore              # see the knowledge graph
   teach me <concept>   # start a lesson
   give me a challenge  # get a quiz
   done                 # mark complete, move on

## Sharing Progress with Friends

1. Create your learner branch:
   git checkout -b learner/your-name
2. Commit progress as you learn:
   git add .learner/
   git commit -m "Progress update"
   git push origin learner/your-name
3. Fetch friends' branches:
   git fetch --all
   friends

## Course Structure

- **Foundations** (4 concepts): Background knowledge including Transformer encoder, language model pre-training, unidirectional limitations, and WordPiece tokenization
- **BERT Core Architecture** (8 concepts): The key architectural components and pre-training objectives including BERT architecture, input representation, MLM, NSP, and more
- **Fine-tuning and Applications** (7 concepts): How BERT is adapted for downstream NLP tasks including classification, QA, NER, and feature extraction
- **Analysis and Impact** (5 concepts): Understanding BERT's design decisions, ablation studies, and its influence on NLP

## Stats

- 24 concepts across 4 courses
- 4 foundational, 9 intermediate, 7 advanced, 4 frontier concepts
