# Direct Preference Optimization: Your Language Model is Secretly a Reward Model — Learning Path

A Claude Code-powered interactive learning path based on
"Direct Preference Optimization: Your Language Model is Secretly a Reward Model" by Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn, 2023.

## Getting Started

1. Open Claude Code in this directory:
   cd direct-preference-optimization-your-language-model/dpo-direct-preference-optimization/
   claude
2. Start learning — just chat naturally:
   explore              # see the knowledge graph
   teach me <concept>   # start a lesson
   give me a challenge  # get a quiz
   done                 # mark complete, move on

## Sharing Progress with Friends

1. Create your learner branch:
   git checkout -b learner/your-name
2. Commit progress as you learn:
   git add .learner/
   git commit -m "Progress update"
   git push origin learner/your-name
3. Fetch friends' branches:
   git fetch --all
   friends

## Course Structure

- **Foundations of Language Model Alignment** (7 concepts): Background concepts needed to understand why DPO exists and the problems it solves
- **The RLHF Pipeline** (4 concepts): Understanding the traditional three-stage RLHF approach that DPO aims to replace
- **DPO: Core Theory and Method** (5 concepts): The mathematical derivation and key insights behind Direct Preference Optimization
- **DPO's Theoretical Properties** (5 concepts): Deep analysis of DPO's mathematical properties, gradient behavior, and advantages over RLHF
- **Frontiers and Applications** (4 concepts): Open research questions, limitations, and future directions for DPO and preference optimization

## Stats

- 25 concepts across 5 courses
- 7 foundational, 8 intermediate,
  5 advanced, 4 frontier concepts
