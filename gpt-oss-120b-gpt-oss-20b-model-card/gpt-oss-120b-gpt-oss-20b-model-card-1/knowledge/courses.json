[
  {
    "id": "foundations",
    "title": "Foundations: Transformers, MoE, and Reasoning",
    "description": "Core concepts needed to understand GPT-OSS architecture and training",
    "concepts": ["transformer_architecture", "mixture_of_experts", "sparse_activation", "chain_of_thought", "knowledge_distillation", "bf16_precision"],
    "lessons": [
      {
        "concept_id": "transformer_architecture",
        "title": "The Transformer Revolution",
        "prerequisites": [],
        "key_ideas": [
          "Self-attention enables parallel sequence processing",
          "Replaces recurrence with attention mechanisms",
          "Foundation for all modern LLMs including GPT-OSS"
        ],
        "code_ref": "",
        "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
        "exercise": "What problem did transformers solve compared to RNNs?\n1) They use less memory\n2) They enable parallel processing of sequences\n3) They have fewer parameters\n4) They train faster on small datasets\nType the number.",
        "explanation": "In 2017, Vaswani and colleagues at Google introduced the Transformer architecture, replacing recurrent connections with self-attention. This was revolutionary because RNNs processed sequences one token at a time—like reading a book word by word without being able to look ahead or back easily.\n\nThe transformer's self-attention mechanism lets every position attend to every other position simultaneously. Imagine a classroom where instead of students whispering messages down a chain (RNN-style), everyone can see and hear everyone else at once.\n\nThis parallel processing ability is why transformers scale so well on modern GPUs, and why they became the foundation for models like GPT-OSS.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "mixture_of_experts",
        "title": "Mixture of Experts: Scaling Without Breaking the Bank",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Multiple specialized expert networks process different inputs",
          "Router selects which experts handle each token",
          "Total parameters far exceed active parameters"
        ],
        "code_ref": "",
        "paper_ref": "Shazeer et al., 2017 — Outrageously Large Neural Networks",
        "exercise": "If a model has 120B total parameters but only 5B active per token, what enables this?\n1) Quantization\n2) Mixture of Experts routing\n3) Knowledge distillation\n4) Flash Attention\nType the number.",
        "explanation": "Shazeer and colleagues proposed Mixture of Experts (MoE) as a way to scale neural networks without proportionally scaling compute costs. The key insight: not all parts of a model need to activate for every input.\n\nThink of it like a hospital with many specialist doctors. When a patient arrives, a triage nurse (the router) directs them to the relevant specialists. The hospital has hundreds of doctors (massive capacity), but each patient only sees a few (sparse activation).\n\nGPT-OSS-120B has 116.8B total parameters but only activates 5.1B per token—roughly 4% of the full model. This allows frontier-scale knowledge while keeping inference costs manageable.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "sparse_activation",
        "title": "Sparse Activation: Doing More with Less",
        "prerequisites": ["mixture_of_experts"],
        "key_ideas": [
          "Only top-K experts activate per token",
          "Enables massive models with bounded compute",
          "Balances capacity with efficiency"
        ],
        "code_ref": "",
        "paper_ref": "Fedus et al., 2022 — Switch Transformers",
        "exercise": "True or False: In sparse activation, increasing the number of experts always increases inference compute cost.\nType True or False.",
        "explanation": "Sparse activation is the principle that makes MoE practical. Instead of using all experts (dense), we activate only a small fixed number (sparse). The Switch Transformer paper showed you could scale to trillions of parameters while keeping compute constant.\n\nImagine a library with 128 librarians, but each book request only needs 4 librarians to find. Adding more librarians doesn't slow down individual requests—it just increases the library's total knowledge.\n\nGPT-OSS uses top-4 routing: regardless of having 128 experts (120B model) or 32 experts (20B model), exactly 4 experts process each token. This is why the 120B model doesn't cost 6x more than the 20B model to run.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "chain_of_thought",
        "title": "Chain-of-Thought: Teaching Models to Think Step-by-Step",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Models verbalize intermediate reasoning steps",
          "Dramatically improves complex task performance",
          "Can be prompted or trained directly"
        ],
        "code_ref": "",
        "paper_ref": "Wei et al., 2022 — Chain-of-Thought Prompting",
        "exercise": "A model is asked '17 * 23 = ?' and outputs '17 * 23 = 17 * 20 + 17 * 3 = 340 + 51 = 391'. This is an example of:\n1) Sparse activation\n2) Chain-of-thought reasoning\n3) Knowledge distillation\n4) Mixture of experts\nType the number.",
        "explanation": "Wei and colleagues at Google discovered that prompting models to 'think step by step' dramatically improved their reasoning abilities. Before CoT, models would often jump to wrong answers on complex problems. With CoT, they break problems into manageable steps.\n\nIt's like the difference between a student who writes just '391' versus one who shows their work: '17 * 20 = 340, 17 * 3 = 51, 340 + 51 = 391'. The explicit steps catch errors and build toward the answer.\n\nGPT-OSS was trained with chain-of-thought RL, learning not just to produce CoT when prompted, but to reason internally. This is foundational to its strong math and coding performance.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "knowledge_distillation",
        "title": "Knowledge Distillation: Learning from the Masters",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Smaller student learns from larger teacher",
          "Teacher provides soft probability distributions",
          "Transfers knowledge while reducing model size"
        ],
        "code_ref": "",
        "paper_ref": "Hinton et al., 2015 — Distilling the Knowledge in a Neural Network",
        "exercise": "In knowledge distillation, what does the student model learn from?\n1) Only the training data\n2) The teacher's output probabilities (soft targets)\n3) The teacher's architecture\n4) Random initialization\nType the number.",
        "explanation": "Hinton and colleagues introduced knowledge distillation as a way to compress large models into smaller ones. The insight: a teacher model's output probabilities contain more information than hard labels alone.\n\nWhen a teacher says 'this is 90% cat, 8% tiger, 2% lion,' it's teaching the student about visual similarity between felines—information lost if we just said 'cat.' The student learns the teacher's nuanced understanding.\n\nGPT-OSS was trained using large-scale distillation from OpenAI's larger proprietary models. This is how relatively smaller open-weight models can achieve near-frontier performance: they're learning from the giants.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "bf16_precision",
        "title": "BF16: The Goldilocks Precision",
        "prerequisites": [],
        "key_ideas": [
          "16-bit format with FP32's dynamic range",
          "Half the memory of FP32",
          "Standard for modern LLM training"
        ],
        "code_ref": "",
        "paper_ref": "Google, 2019 — BFloat16 specification",
        "exercise": "Why is BF16 preferred over FP16 for training large models?\n1) BF16 uses less memory\n2) BF16 has the same exponent range as FP32, preventing overflow\n3) BF16 is more precise than FP16\n4) BF16 is faster to compute\nType the number.",
        "explanation": "Brain floating-point 16 (BF16) emerged from Google's TPU work as a compromise between precision and efficiency. FP16 has more mantissa bits (precision) but fewer exponent bits (range). BF16 trades precision for range.\n\nThink of exponent bits as the 'zoom level' of a camera. FP16 can zoom in close but can't capture very bright or dark scenes without clipping. BF16's wider range handles the extreme values that occur during training.\n\nGPT-OSS supports BF16 inference, striking a balance between the memory savings of 16-bit and the stability needed for reliable model outputs.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "gpt_oss_core",
    "title": "GPT-OSS Architecture Deep Dive",
    "description": "Understanding the specific architectural choices and training methods of GPT-OSS",
    "concepts": ["gpt_oss_architecture", "top_k_routing", "swiglu_activation", "cot_rl_training", "large_scale_distillation", "o200k_harmony_tokenizer", "harmony_format"],
    "lessons": [
      {
        "concept_id": "gpt_oss_architecture",
        "title": "Inside GPT-OSS: Architecture Overview",
        "prerequisites": ["mixture_of_experts", "sparse_activation"],
        "key_ideas": [
          "120B model: 116.8B total, 5.1B active, 128 experts",
          "20B model: 20.9B total, 3.6B active, 32 experts",
          "2,880-dimensional residual streams with top-4 routing"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "The GPT-OSS-120B model has 128 experts but uses top-4 routing. What percentage of experts activate per token?\n1) 100%\n2) 25%\n3) About 3%\n4) 50%\nType the number.",
        "explanation": "In August 2025, OpenAI released GPT-OSS as their first open-weight reasoning models. The architecture represents a careful balance between capability and efficiency.\n\nThe 120B model packs 116.8 billion parameters into 128 expert networks, but each token only routes through 4 experts, using just 5.1B active parameters. That's about 3% of the model—like having access to a 128-person legal team but only needing 4 specialists per case.\n\nThe smaller 20B model uses 32 experts with the same top-4 routing, activating 3.6B of its 20.9B parameters. Both share a 2,880-dimensional residual stream and gated SwiGLU activations.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "top_k_routing",
        "title": "Top-K Routing: Choosing the Right Experts",
        "prerequisites": ["sparse_activation", "gpt_oss_architecture"],
        "key_ideas": [
          "Linear projection computes expert scores",
          "Top 4 experts selected per token",
          "Balances specialization with robustness"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "In top-K routing, how are experts selected?\n1) Randomly\n2) Round-robin cycling\n3) By highest router projection scores\n4) Based on token position\nType the number.",
        "explanation": "The router is the 'air traffic controller' of an MoE model. For each token, a linear projection produces a score for every expert. The top-K (in GPT-OSS, K=4) experts with highest scores process that token.\n\nThink of it like a TV talent show with 128 judges. Each contestant (token) presents their act, and the top 4 judges who 'buzz in' fastest (highest scores) get to evaluate them.\n\nWhy top-4 instead of top-1 or top-8? It's a tradeoff. Top-1 (Switch Transformer style) is most efficient but can be unstable. Top-4 provides redundancy—if one expert makes an error, others can compensate—while keeping compute bounded.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "swiglu_activation",
        "title": "SwiGLU: The Activation Function of Choice",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Combines Swish activation with gating",
          "Better gradient flow than ReLU/GELU",
          "Standard in modern LLMs like LLaMA and GPT-OSS"
        ],
        "code_ref": "",
        "paper_ref": "Shazeer, 2020 — GLU Variants Improve Transformer",
        "exercise": "SwiGLU combines which two components?\n1) ReLU and softmax\n2) Swish activation and gating mechanism\n3) Attention and feedforward\n4) Layer norm and dropout\nType the number.",
        "explanation": "Noam Shazeer (yes, the same researcher behind MoE and many other innovations) proposed SwiGLU as an improved activation function. It combines the smooth Swish activation (x * sigmoid(x)) with a gating mechanism from Gated Linear Units.\n\nImagine a valve that doesn't just open or close (ReLU), but smoothly controls flow AND decides what to let through. The gating part learns what information matters; the Swish part transforms it smoothly.\n\nSwiGLU requires 50% more parameters than ReLU for the same layer width, but the improved gradient flow and expressiveness make it worthwhile. GPT-OSS uses gated SwiGLU in all its expert networks.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "cot_rl_training",
        "title": "Training Reasoning with RL",
        "prerequisites": ["chain_of_thought", "gpt_oss_architecture"],
        "key_ideas": [
          "RL rewards correct reasoning and final answers",
          "Similar techniques to OpenAI o3",
          "Teaches both reasoning and tool usage"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "What does CoT RL training optimize for?\n1) Faster inference speed\n2) Smaller model size\n3) Correct reasoning chains and final answers\n4) More expert utilization\nType the number.",
        "explanation": "GPT-OSS wasn't just pre-trained on text—it was post-trained using reinforcement learning to reason and use tools. OpenAI describes this as using 'similar CoT RL techniques as OpenAI o3.'\n\nTraditional supervised learning shows the model correct answers. RL goes further: the model generates its own reasoning chains, receives rewards for correct final answers, and learns which reasoning strategies work. It's like learning chess by playing games rather than studying solutions.\n\nThis RL training is what enables GPT-OSS to adjust its reasoning effort (low/medium/high) and to interleave tool calls within its chain of thought—capabilities that emerge from optimizing for task completion, not just text prediction.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "large_scale_distillation",
        "title": "Learning from Giants: Large-Scale Distillation",
        "prerequisites": ["knowledge_distillation", "gpt_oss_architecture"],
        "key_ideas": [
          "Distillation from larger proprietary models",
          "Combined with CoT RL fine-tuning",
          "Enables competitive open-weight models"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "True or False: GPT-OSS was trained entirely from scratch without any knowledge transfer from larger models.\nType True or False.",
        "explanation": "GPT-OSS achieves near-frontier performance despite being open-weight through large-scale distillation. The models learned from OpenAI's larger, more capable proprietary models before being released publicly.\n\nThis is like an apprentice learning from a master craftsman. The apprentice (GPT-OSS) may be smaller, but they've absorbed the master's techniques and intuitions. Combined with CoT RL, the models develop their own refined reasoning abilities.\n\nThe training process consumed 2.1 million H100-hours for the larger model—massive compute, but far less than training a frontier model from scratch. Distillation is the shortcut that makes open-weight releases feasible.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "o200k_harmony_tokenizer",
        "title": "The o200k_harmony Tokenizer",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "201,088 total vocabulary tokens",
          "Extends GPT-4o's o200k tokenizer",
          "Special tokens for harmony chat format"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "The o200k_harmony tokenizer has how many tokens?\n1) 50,000\n2) 100,000\n3) About 200,000\n4) 1,000,000\nType the number.",
        "explanation": "Tokenizers convert text into numbers that models process. GPT-OSS uses the o200k_harmony tokenizer—an extension of the o200k tokenizer from GPT-4o and o1-mini, available via OpenAI's TikToken library.\n\nThe '200k' refers to the ~200,000 base vocabulary tokens learned via Byte Pair Encoding. The '_harmony' suffix indicates additional special tokens for the chat format: markers for message boundaries, roles, and channels.\n\nThink of the tokenizer as a very sophisticated compression algorithm. Common words like 'the' get single tokens, while rare technical terms might need multiple. The 200k+ vocabulary ensures efficient encoding across many languages and domains.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "harmony_format",
        "title": "Harmony Format: Structured Conversations",
        "prerequisites": ["o200k_harmony_tokenizer"],
        "key_ideas": [
          "Special tokens delineate message boundaries",
          "Roles: User, Assistant, System, Developer",
          "Channels: analysis (CoT), commentary (tools), final (answer)"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "In the Harmony format, which channel contains the model's chain-of-thought reasoning?\n1) final\n2) commentary\n3) analysis\n4) user\nType the number.",
        "explanation": "The Harmony format is GPT-OSS's native conversation structure. Unlike simple prompt templates, it uses special tokens to explicitly mark message boundaries, roles, and visibility channels.\n\nThe three channels are key: 'analysis' contains internal reasoning (chain-of-thought), 'commentary' holds tool calls and their results, and 'final' is the user-visible answer. This separation lets the model think extensively while showing users only the polished result.\n\nThink of it like a stage production: analysis is the rehearsal, commentary is backstage coordination, and final is the performance the audience sees. The format supports interleaved tool calls within CoT—the model can pause reasoning, use a tool, incorporate results, and continue.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "reasoning_and_tools",
    "title": "Reasoning, Tools, and Agentic Capabilities",
    "description": "How GPT-OSS thinks, uses tools, and acts as an autonomous agent",
    "concepts": ["reasoning_effort_levels", "test_time_compute", "agentic_capabilities", "browsing_tool", "python_tool", "function_calling"],
    "lessons": [
      {
        "concept_id": "reasoning_effort_levels",
        "title": "Configurable Reasoning: Low, Medium, High",
        "prerequisites": ["chain_of_thought", "harmony_format"],
        "key_ideas": [
          "Three levels controlled via system prompt",
          "Higher effort = longer CoT = better accuracy",
          "Log-linear returns on accuracy vs compute"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "As reasoning effort increases from low to high, what happens?\n1) Accuracy decreases but speed increases\n2) Accuracy increases with diminishing (log-linear) returns\n3) Accuracy stays the same but cost decreases\n4) The model uses more experts\nType the number.",
        "explanation": "GPT-OSS lets you choose how hard the model thinks. Set 'Reasoning: low' in the system prompt for quick answers, 'medium' for balanced performance, or 'high' for maximum accuracy.\n\nThe tradeoff follows log-linear scaling: doubling reasoning time gives a fixed accuracy boost, not a doubling of accuracy. It's like studying for an exam—the first hour helps a lot, the tenth hour less so.\n\nThis configurability is powerful for production systems. Quick factual queries can use 'low' for speed; complex math problems deserve 'high.' AIME benchmark scores jump from baseline to 95.8% (120B) with high effort and tools.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "test_time_compute",
        "title": "Test-Time Compute: Thinking Harder at Inference",
        "prerequisites": ["reasoning_effort_levels"],
        "key_ideas": [
          "More inference compute improves accuracy",
          "Orthogonal to pre-training scale",
          "Enables dynamic cost/accuracy tradeoffs"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Test-time compute scaling is orthogonal to model size scaling. This means:\n1) They're the same thing\n2) You can improve accuracy by scaling either independently\n3) Larger models can't benefit from test-time compute\n4) Test-time compute only works for small models\nType the number.",
        "explanation": "Traditional scaling focuses on training: bigger models, more data, more compute during pre-training. Test-time compute scaling is the complementary insight that you can also improve by thinking harder at inference time.\n\nGPT-OSS demonstrates smooth test-time scaling: given the same model, longer chain-of-thought reasoning produces better answers. This is revolutionary for deployment—you don't need different sized models for different accuracy requirements, just different reasoning effort settings.\n\nIt's like having a brilliant consultant who can give quick takes or deep analyses depending on how much time you give them. The underlying expertise is the same; the depth of consideration varies.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "agentic_capabilities",
        "title": "Agentic AI: Tools, Browsing, and Autonomous Action",
        "prerequisites": ["harmony_format", "cot_rl_training"],
        "key_ideas": [
          "Models invoke external tools during reasoning",
          "Browsing, code execution, custom functions",
          "Enables multi-step autonomous problem solving"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "What makes GPT-OSS 'agentic'?\n1) It has human-like emotions\n2) It can use tools and take actions autonomously\n3) It runs on multiple GPUs\n4) It has more parameters than other models\nType the number.",
        "explanation": "GPT-OSS isn't just a text predictor—it's an agent that can take actions. During reasoning, it can search the web, execute Python code, or call custom developer functions. This transforms it from a knowledge base into an autonomous problem solver.\n\nThe key insight from CoT RL training: the model learned to interleave tool use with reasoning. It might start solving a math problem, realize it needs a calculation, invoke the Python tool, incorporate the result, and continue reasoning.\n\nThis agentic capability is what enables SWE-Bench performance (62.4%): the model doesn't just suggest fixes, it explores codebases, runs tests, and generates patches that actually work.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "browsing_tool",
        "title": "Browsing: Real-Time Information Retrieval",
        "prerequisites": ["agentic_capabilities"],
        "key_ideas": [
          "Search function for web queries",
          "Open function for reading pages",
          "Integrates with chain-of-thought"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "When would the browsing tool be most useful?\n1) Solving a math equation\n2) Answering questions about recent events\n3) Generating creative fiction\n4) Correcting grammar\nType the number.",
        "explanation": "The browsing tool gives GPT-OSS access to real-time information. When reasoning about current events, recent research, or live data, the model can search the web and read pages—no more 'my knowledge was last updated in...' limitations.\n\nTwo main functions: 'search' submits queries and returns results, 'open' retrieves full page content. The model decides when to browse based on the question, seamlessly integrating web information into its reasoning chain.\n\nFor research tasks, this is transformative. The model can look up papers, verify facts, and gather information, essentially conducting research like a human would. Deep research benchmarks show competitive performance because the model can actually go find answers.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "python_tool",
        "title": "Python Tool: Computational Reasoning",
        "prerequisites": ["agentic_capabilities"],
        "key_ideas": [
          "Stateful Jupyter notebook environment",
          "Variables persist across executions",
          "Enables precise computation and data analysis"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "The Python tool is 'stateful'. This means:\n1) It runs in different US states\n2) Variables persist between multiple code executions\n3) It only works with state machines\n4) Each execution starts fresh with no memory\nType the number.",
        "explanation": "The Python tool provides a Jupyter notebook-style environment where GPT-OSS can write and execute code. Crucially, it's stateful—variables from one code block persist to the next.\n\nThis matters for complex computations. The model might load data in one block, process it in another, and visualize in a third. Without statefulness, it would need to repeat setup each time.\n\nFor math benchmarks like AIME, the Python tool is game-changing. Instead of risking arithmetic errors in reasoning, the model can verify calculations: '17 * 23... let me compute that: `print(17 * 23)` → 391'. This is why scores jump significantly with tool access.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "function_calling",
        "title": "Developer Functions: Custom Tool Integration",
        "prerequisites": ["agentic_capabilities", "harmony_format"],
        "key_ideas": [
          "Developers define function schemas",
          "Model generates structured function calls",
          "Enables arbitrary external system integration"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Developer function calling enables:\n1) Only browsing the web\n2) Only executing Python\n3) Integration with arbitrary external systems via defined schemas\n4) Faster model inference\nType the number.",
        "explanation": "Beyond built-in tools, GPT-OSS supports custom developer functions. You define a function schema in a Developer message—name, parameters, description—and the model can call it during reasoning.\n\nThis is the extension point for building on GPT-OSS. Need it to query your database? Define a `query_db` function. Control IoT devices? Define `set_temperature`. The model doesn't execute these directly; it generates structured calls that your application handles.\n\nFunction calling turns GPT-OSS into a reasoning engine that can interface with any system. It's the bridge between AI reasoning and real-world actions.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "optimization_and_deployment",
    "title": "Optimization for Efficient Deployment",
    "description": "Making GPT-OSS fast and memory-efficient for real-world use",
    "concepts": ["flash_attention", "triton_kernels", "mxfp4_quantization", "safety_training", "cbrn_filters"],
    "lessons": [
      {
        "concept_id": "flash_attention",
        "title": "Flash Attention: Memory-Efficient Attention",
        "prerequisites": ["transformer_architecture", "bf16_precision"],
        "key_ideas": [
          "Tiled computation avoids O(n^2) memory",
          "Fuses operations for GPU efficiency",
          "Essential for long-context models"
        ],
        "code_ref": "",
        "paper_ref": "Dao et al., 2022 — FlashAttention",
        "exercise": "Flash Attention reduces memory usage from O(n^2) to O(n) by:\n1) Using smaller models\n2) Computing attention in tiles without materializing the full matrix\n3) Reducing the number of attention heads\n4) Compressing the input sequence\nType the number.",
        "explanation": "Standard attention computes a full n×n matrix for a sequence of length n, requiring O(n²) memory. For long sequences, this becomes prohibitive. Tri Dao's Flash Attention revolutionized this by computing attention in tiles.\n\nThe key insight: you don't need to store the entire attention matrix if you compute it in blocks and accumulate results. It's like computing a huge multiplication by breaking it into smaller pieces that fit in fast memory.\n\nFlash Attention also fuses multiple operations into single GPU kernels, maximizing hardware utilization. GPT-OSS's training used Flash Attention algorithms to make 2.1M H100-hours feasible.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "triton_kernels",
        "title": "Triton: Python-like GPU Programming",
        "prerequisites": ["flash_attention"],
        "key_ideas": [
          "High-level language for GPU kernels",
          "Automatic memory optimization",
          "Custom operations without CUDA expertise"
        ],
        "code_ref": "",
        "paper_ref": "Tillet et al., 2019 — Triton",
        "exercise": "Triton enables:\n1) Training without GPUs\n2) Writing custom GPU kernels in a Python-like language\n3) Using only CPUs for inference\n4) Automatic model architecture design\nType the number.",
        "explanation": "Writing efficient GPU code traditionally required CUDA expertise—managing threads, memory, synchronization. Triton provides a Python-like language that handles these details automatically.\n\nDevelopers write high-level operations (matrix multiplies, reductions), and Triton compiles them to optimized GPU code. It handles memory coalescing, tiling, and parallelization—the tedious parts that cause performance bugs.\n\nGPT-OSS's training used PyTorch with Triton kernels for custom operations. This combination lets researchers iterate quickly while maintaining GPU efficiency, crucial for the 2.1M H100-hour training run.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "mxfp4_quantization",
        "title": "MXFP4: Extreme Compression for Deployment",
        "prerequisites": ["bf16_precision", "gpt_oss_architecture"],
        "key_ideas": [
          "4.25 bits per parameter on average",
          "Applied to MoE weights (90%+ of model)",
          "120B fits on 80GB GPU, 20B on 16GB"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "MXFP4 quantization compresses weights to:\n1) 1 bit per parameter\n2) About 4 bits per parameter\n3) 8 bits per parameter\n4) 16 bits per parameter\nType the number.",
        "explanation": "MXFP4 (Microscaling FP4) quantization compresses model weights to just 4.25 bits per parameter on average. Applied to the MoE expert weights (which comprise 90%+ of parameters), this dramatically reduces memory requirements.\n\nThe results are remarkable: GPT-OSS-120B, with 116.8B parameters, fits on a single 80GB GPU. The 20B model runs on consumer hardware with just 16GB memory. This is the difference between needing a data center and running on a gaming PC.\n\nGPT-OSS was 'post-trained with MXFP4 quantization'—meaning the model learned to work with quantized weights during training, not just as a post-hoc compression. All evaluations used the same quantization, ensuring real-world numbers match benchmarks.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "safety_training",
        "title": "Safety Training: Helpful but Harmless",
        "prerequisites": ["cot_rl_training"],
        "key_ideas": [
          "Models refuse disallowed content categories",
          "Adversarial fine-tuning tested dangerous capabilities",
          "Instruction hierarchy prevents jailbreaks"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "What does safety training aim to prevent?\n1) Slow inference\n2) High memory usage\n3) Generation of harmful content\n4) Overfitting to training data\nType the number.",
        "explanation": "Safety training teaches models to refuse harmful requests while remaining helpful. GPT-OSS achieves 99.6% accuracy refusing hate speech and 100% on sexual/minors content in the Standard Disallowed Content benchmark.\n\nThe approach combines multiple strategies: content classifiers during training, RL that rewards refusals of harmful requests, and instruction hierarchy training to resist jailbreak attempts. OpenAI also conducted adversarial fine-tuning to test if safety could be easily removed.\n\nFor open-weight models, safety training is crucial because developers can't revoke access. The safety behaviors must be robust enough to survive diverse deployment contexts.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "cbrn_filters",
        "title": "CBRN Filters: Preventing Dangerous Knowledge",
        "prerequisites": ["safety_training"],
        "key_ideas": [
          "Removes weapons-related content from training data",
          "Covers chemical, biological, radiological, nuclear domains",
          "Part of responsible scaling practices"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "What does CBRN stand for in the context of AI safety?\n1) Cross-Border Research Networks\n2) Chemical, Biological, Radiological, Nuclear\n3) Compute-Bound Resource Networks\n4) Chain-Based Reasoning Nodes\nType the number.",
        "explanation": "CBRN (Chemical, Biological, Radiological, Nuclear) filters remove weapons-related content from training data. This prevents models from learning detailed instructions for creating dangerous materials.\n\nOpenAI's Safety Advisory Group evaluated GPT-OSS for CBRN capabilities. The models 'did not reach High capability thresholds in biological/chemical or cyber domains'—meaning they don't provide meaningful uplift for creating dangerous weapons.\n\nThis is a key difference from pure capability research. Responsible AI development acknowledges that some knowledge is too dangerous to encode in open-weight models. CBRN filters are one implementation of this principle.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "benchmarks_and_future",
    "title": "Performance, Evaluation, and Future Directions",
    "description": "How GPT-OSS performs and where the field is heading",
    "concepts": ["swe_bench", "aime_benchmark", "open_weights", "edge_deployment"],
    "lessons": [
      {
        "concept_id": "swe_bench",
        "title": "SWE-Bench: Real-World Software Engineering",
        "prerequisites": ["agentic_capabilities", "python_tool"],
        "key_ideas": [
          "Tests ability to fix real GitHub issues",
          "Models must understand and patch actual bugs",
          "GPT-OSS: 62.4% (120B), 60.7% (20B)"
        ],
        "code_ref": "",
        "paper_ref": "Jimenez et al., 2024 — SWE-bench",
        "exercise": "SWE-Bench tests models on:\n1) Writing documentation\n2) Solving real GitHub issues by generating code patches\n3) Designing system architectures\n4) Code review\nType the number.",
        "explanation": "SWE-Bench (Software Engineering Bench) is a benchmark that gives models real GitHub issues and asks them to generate patches that fix the bugs. Unlike synthetic coding tasks, these are actual problems from popular repositories.\n\nGPT-OSS-120B achieves 62.4% on the verified subset—meaning it correctly fixes 62% of real-world bugs when given the issue description and codebase. This requires reading code, understanding the bug, reasoning about the fix, and generating correct patches.\n\nThe 20B model at 60.7% shows the smaller model is remarkably capable—only 2% behind on this complex task. This is the MoE efficiency advantage in action: fewer active parameters, nearly identical performance.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "aime_benchmark",
        "title": "AIME: Competition-Level Mathematics",
        "prerequisites": ["reasoning_effort_levels", "python_tool"],
        "key_ideas": [
          "American Invitational Mathematics Examination problems",
          "Tests multi-step mathematical reasoning",
          "GPT-OSS: 95.8% (120B), 92.1% (20B) with tools + high effort"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "GPT-OSS-120B scores 95.8% on AIME 2024 with tools and high reasoning effort. What does this suggest?\n1) The model memorized the answers\n2) High reasoning effort and tools significantly boost math performance\n3) AIME problems are too easy\n4) The benchmark is unreliable\nType the number.",
        "explanation": "AIME (American Invitational Mathematics Examination) is a prestigious high school math competition with problems requiring creative multi-step reasoning. GPT-OSS-120B achieves 95.8% on AIME 2024—competition-winning performance.\n\nThe score requires both high reasoning effort (longer chain-of-thought) and tool access (Python for computation). Without these, scores are significantly lower. This demonstrates that model capability alone isn't enough; how you use the model matters.\n\nThe 20B model at 92.1% shows that even the smaller model is an exceptional mathematician. The few percent difference between sizes suggests the core reasoning strategies transfer well through distillation.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "open_weights",
        "title": "Open Weights: Democratizing Frontier AI",
        "prerequisites": ["gpt_oss_architecture", "mxfp4_quantization"],
        "key_ideas": [
          "Apache 2.0 license allows commercial use",
          "Enables on-premise and edge deployment",
          "Shifts control from API providers to users"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Why is Apache 2.0 licensing significant for GPT-OSS?\n1) It means the model is free to use but closed-source\n2) It allows commercial use, modification, and distribution\n3) It restricts use to research only\n4) It requires sharing all derivative work\nType the number.",
        "explanation": "OpenAI's release of GPT-OSS under Apache 2.0 represents a strategic shift. Unlike API-based models, open weights give users full control: deploy on-premise, fine-tune for specific domains, integrate into products, even modify the architecture.\n\nThis democratizes frontier AI capabilities. A startup can now compete with tech giants' AI features using the same underlying technology. Researchers can study model internals. Privacy-sensitive applications can run entirely locally.\n\nThe tradeoff: OpenAI loses the ability to revoke access or update the model post-deployment. That's why safety training and CBRN filters are applied before release—once the weights are public, they're public forever.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "edge_deployment",
        "title": "Edge Deployment: AI Without the Cloud",
        "prerequisites": ["mxfp4_quantization", "open_weights"],
        "key_ideas": [
          "20B model runs on 16GB consumer hardware",
          "No internet or API required",
          "Enables privacy-preserving local inference"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Edge deployment of GPT-OSS-20B is enabled primarily by:\n1) Faster internet connections\n2) MXFP4 quantization reducing memory requirements\n3) Smaller vocabulary size\n4) Fewer attention layers\nType the number.",
        "explanation": "MXFP4 quantization enables GPT-OSS-20B to run on systems with just 16GB of memory. This includes gaming laptops, high-end desktops, and edge devices—no data center required.\n\nEdge deployment changes the economics and privacy model of AI. Inference is free after the initial download. Data never leaves the device. Latency is bounded by local compute, not network round-trips.\n\nFor applications like healthcare, legal, or personal assistants, this is transformative. A doctor can use frontier-level AI reasoning without sending patient data to external servers. A company can deploy AI capabilities without per-token API costs.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  }
]
