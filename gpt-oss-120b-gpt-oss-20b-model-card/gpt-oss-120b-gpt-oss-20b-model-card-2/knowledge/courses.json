[
  {
    "id": "foundations",
    "title": "Foundations: Transformers, MoE, and Training",
    "description": "Essential background on transformer architecture, mixture of experts, and training paradigms that underpin GPT-OSS",
    "concepts": [
      "transformer_architecture",
      "mixture_of_experts",
      "swiglu_activation",
      "chain_of_thought",
      "knowledge_distillation",
      "reinforcement_learning_from_human_feedback"
    ],
    "lessons": [
      {
        "concept_id": "transformer_architecture",
        "title": "The Transformer Revolution",
        "prerequisites": [],
        "key_ideas": [
          "Transformers use self-attention instead of recurrence to model sequences",
          "Autoregressive models predict the next token given all previous tokens",
          "GPT-OSS builds on the decoder-only design from GPT-2 and GPT-3"
        ],
        "code_ref": "",
        "paper_ref": "Vaswani et al., 2017 — Attention Is All You Need",
        "exercise": "What is the key innovation that allows transformers to process all positions in parallel?\n1) Recurrent connections\n2) Self-attention mechanism\n3) Convolutional filters\nType the number.",
        "explanation": "In 2017, Vaswani et al. at Google introduced the Transformer in 'Attention Is All You Need.' Before this, sequence models like LSTMs processed tokens one at a time, creating a bottleneck. The breakthrough was self-attention: every position can directly attend to every other position in one step.\n\nThink of it like a classroom discussion vs. a game of telephone. RNNs pass information down a chain (telephone), while transformers let everyone hear everyone else simultaneously (discussion).\n\nGPT-OSS uses the autoregressive transformer design: it predicts the next token based on all previous tokens, generating text left-to-right just like GPT-2 and GPT-3.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "mixture_of_experts",
        "title": "Mixture of Experts: Scaling Smart",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "MoE uses multiple specialized sub-networks called experts",
          "A router network decides which experts process each token",
          "Only a subset of parameters are active, enabling massive models with controlled compute"
        ],
        "code_ref": "",
        "paper_ref": "Shazeer et al., 2017 — Outrageously Large Neural Networks",
        "exercise": "Why is MoE computationally efficient despite having many parameters?\n1) All experts run in parallel\n2) Only selected experts activate per token\n3) Experts share weights\nType the number.",
        "explanation": "Shazeer et al. at Google introduced Mixture of Experts in 2017 to break the scaling limits of dense models. The insight: not every input needs every parameter.\n\nImagine a hospital with 100 specialist doctors. Each patient only sees 2-3 relevant specialists, not all 100. The hospital has massive expertise (parameters), but each visit (forward pass) is efficient.\n\nMoE transformers have multiple 'expert' feed-forward networks. A learned router picks which experts process each token. GPT-OSS-120b has 116.8B total parameters but only 5.1B are active per token—6x efficiency gain!",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "swiglu_activation",
        "title": "SwiGLU: The Better Activation",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "SwiGLU combines Swish activation with Gated Linear Units",
          "Gating lets the network learn what information to pass through",
          "GPT-OSS adds unconventional clamping for numerical stability"
        ],
        "code_ref": "",
        "paper_ref": "Shazeer, 2020 — GLU Variants Improve Transformer",
        "exercise": "What does the 'gating' in SwiGLU do?\n1) Limits the maximum output value\n2) Learns what information to pass through\n3) Speeds up training\nType the number.",
        "explanation": "Noam Shazeer (again!) showed in 2020 that GLU variants outperform standard activations like ReLU and GELU in transformers.\n\nSwiGLU works like this: instead of just transforming the input, the network produces two outputs—one is the 'candidate' and one is the 'gate.' The gate (passed through sigmoid-like Swish) controls how much of the candidate passes through.\n\nThink of it as a smart dimmer switch that learns which signals to amplify and which to suppress. GPT-OSS uses SwiGLU with additional clamping to prevent numerical overflow during training.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "chain_of_thought",
        "title": "Chain-of-Thought: Thinking Out Loud",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "CoT prompts the model to generate intermediate reasoning steps",
          "Explicit reasoning dramatically improves complex problem solving",
          "GPT-OSS provides full CoT access for transparency"
        ],
        "code_ref": "",
        "paper_ref": "Wei et al., 2022 — Chain-of-Thought Prompting Elicits Reasoning in LLMs",
        "exercise": "True or False: Chain-of-thought reasoning only works with special fine-tuning.\nType 'True' or 'False'.",
        "explanation": "Wei et al. at Google discovered in 2022 that simply asking models to 'think step by step' unlocks reasoning abilities that direct Q&A cannot.\n\nIt's like the difference between asking someone 'What's 17 × 24?' (answer only) versus 'Show your work for 17 × 24' (reasoning). The latter forces breaking down the problem.\n\nGPT-OSS is trained with CoT reinforcement learning—it learns not just to reason, but when and how much to reason. Crucially, OpenAI releases full CoT access so developers can see exactly how the model thinks.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "knowledge_distillation",
        "title": "Knowledge Distillation: Teaching Smaller Models",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "A smaller student model learns to mimic a larger teacher",
          "Soft labels from the teacher contain more information than hard labels",
          "GPT-OSS uses large-scale distillation in training"
        ],
        "code_ref": "",
        "paper_ref": "Hinton et al., 2015 — Distilling the Knowledge in a Neural Network",
        "exercise": "What advantage do 'soft labels' from a teacher model provide?\n1) They're faster to compute\n2) They contain richer information about class relationships\n3) They use less memory\nType the number.",
        "explanation": "Hinton et al. introduced knowledge distillation in 2015. The key insight: a large teacher model's output probabilities (soft labels) contain more information than one-hot labels.\n\nIf a teacher says an image is '70% cat, 25% dog, 5% tiger,' that tells the student these animals are related—information lost in a hard 'cat' label.\n\nGPT-OSS training uses large-scale distillation—likely training on outputs from larger OpenAI models. This transfers reasoning patterns efficiently to the smaller open models.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "reinforcement_learning_from_human_feedback",
        "title": "RLHF: Learning from Human Preferences",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Human evaluators rank model outputs by preference",
          "A reward model learns to predict human preferences",
          "RL (often PPO) optimizes the language model against this reward"
        ],
        "code_ref": "",
        "paper_ref": "Ouyang et al., 2022 — Training language models to follow instructions with human feedback",
        "exercise": "In RLHF, what does the reward model predict?\n1) The next token probability\n2) Human preference rankings\n3) Grammar correctness\nType the number.",
        "explanation": "OpenAI's InstructGPT paper (Ouyang et al., 2022) showed how to align models with human intent using RLHF. The problem: defining 'good' outputs mathematically is hard.\n\nThe solution: collect human comparisons (Response A > Response B), train a reward model to predict these preferences, then use RL to maximize predicted reward.\n\nThink of it as training a dog: you can't explain 'sit' in dog language, but you can reward good behavior. GPT-OSS uses RL extensively for reasoning capabilities and tool use—the same technique behind ChatGPT and Claude.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "architecture_deep_dive",
    "title": "GPT-OSS Architecture Deep Dive",
    "description": "Understanding the specific architectural choices in GPT-OSS: attention variants, context extension, and efficiency techniques",
    "concepts": [
      "moe_transformer_gpt_oss",
      "grouped_query_attention",
      "yarn_context_extension",
      "banded_window_attention",
      "flash_attention",
      "o200k_harmony_tokenizer"
    ],
    "lessons": [
      {
        "concept_id": "moe_transformer_gpt_oss",
        "title": "The GPT-OSS MoE Architecture",
        "prerequisites": ["transformer_architecture", "mixture_of_experts"],
        "key_ideas": [
          "GPT-OSS-120b: 116.8B total params, 5.1B active, 128 experts, top-4 routing",
          "GPT-OSS-20b: 20.9B total params, 3.6B active, 32 experts, top-4 routing",
          "Linear router projection selects which experts handle each token"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "If GPT-OSS-120b has 116.8B params but only 5.1B active, what's the approximate efficiency ratio?\n1) About 5x more params than compute\n2) About 20x more params than compute\n3) About 2x more params than compute\nType the number.",
        "explanation": "OpenAI's GPT-OSS implements MoE at massive scale. The 120b model packs 116.8 billion parameters into 128 experts, but each token only activates the top 4 experts selected by a learned linear router.\n\nThis means only 5.1B parameters are 'active' per forward pass—roughly 23x efficiency gain! The 20b model uses 32 experts with 3.6B active out of 20.9B total.\n\nThink of it as a library with 128 subject sections. Each query only needs 4 relevant sections, not the entire library. The router learns which 'sections' to consult for each input.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "grouped_query_attention",
        "title": "Grouped Query Attention (GQA)",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Multiple query heads share the same key-value heads",
          "GPT-OSS uses 64 query heads with only 8 KV heads (8:1 ratio)",
          "Reduces KV cache memory without significant quality loss"
        ],
        "code_ref": "",
        "paper_ref": "Ainslie et al., 2023 — GQA: Training Generalized Multi-Query Transformer Models",
        "exercise": "GQA primarily reduces memory usage during:\n1) Training\n2) Inference (KV cache)\n3) Model loading\nType the number.",
        "explanation": "Standard multi-head attention gives each query head its own key-value heads. This means the KV cache (stored during generation) grows linearly with heads.\n\nAinslie et al. proposed GQA: group query heads to share KV heads. GPT-OSS uses 64 query heads but only 8 KV heads—8 queries share each KV pair.\n\nImagine 64 students (queries) and 8 textbooks (KV). Instead of 64 textbooks, groups of 8 students share a textbook. Same learning capacity, 8x less library space (memory). This is crucial for long-context inference on limited hardware.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "yarn_context_extension",
        "title": "YaRN: Stretching Context Windows",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "YaRN modifies rotary position embeddings for longer sequences",
          "Enables 131,072 token context without full retraining",
          "Applied to dense attention layers in GPT-OSS"
        ],
        "code_ref": "",
        "paper_ref": "Peng et al., 2023 — YaRN: Efficient Context Window Extension of LLMs",
        "exercise": "YaRN extends context length by modifying:\n1) The vocabulary size\n2) Rotary position embeddings\n3) The number of attention heads\nType the number.",
        "explanation": "Models trained on 4K tokens struggle with 128K inputs because position embeddings weren't seen during training. YaRN (Yet another RoPE Naturally) by Peng et al. solves this elegantly.\n\nRoPE encodes positions using rotation angles. YaRN interpolates these rotations, 'stretching' learned positions to cover longer sequences. It's like teaching someone to count 1-100, then explaining how to extrapolate to 1000.\n\nGPT-OSS uses YaRN to achieve 131,072 token context—enough to process entire codebases or books in a single prompt.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "banded_window_attention",
        "title": "Banded Window Attention",
        "prerequisites": ["transformer_architecture", "grouped_query_attention"],
        "key_ideas": [
          "Local attention within fixed 128-token windows",
          "Alternates with dense global attention layers",
          "Reduces O(n²) complexity for long sequences"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "What's the main benefit of alternating banded and dense attention?\n1) Simpler implementation\n2) Local efficiency with global context preservation\n3) Better gradient flow\nType the number.",
        "explanation": "Full attention is O(n²)—128K tokens means 16 billion attention computations per layer! GPT-OSS uses a hybrid approach.\n\nBanded layers only attend within a 128-token sliding window (local context). Dense layers attend to everything (global context). By alternating, the model gets efficiency on most layers while maintaining global understanding.\n\nThink of reading a book: you focus on nearby words (local) but periodically zoom out to remember chapter themes (global). This pattern achieves the best of both worlds.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "flash_attention",
        "title": "Flash Attention: Memory-Efficient Attention",
        "prerequisites": ["transformer_architecture"],
        "key_ideas": [
          "Fuses attention operations to reduce memory I/O",
          "Uses tiling to fit computation in fast SRAM",
          "Enables longer sequences without OOM errors"
        ],
        "code_ref": "",
        "paper_ref": "Dao et al., 2022 — FlashAttention: Fast and Memory-Efficient Exact Attention",
        "exercise": "Flash Attention achieves speedups primarily by reducing:\n1) The number of computations\n2) Memory I/O operations\n3) Model parameters\nType the number.",
        "explanation": "Tri Dao's Flash Attention (2022) was a paradigm shift. Standard attention writes huge intermediate matrices to GPU memory (HBM), creating I/O bottlenecks.\n\nFlash Attention tiles the computation: process attention in blocks that fit in fast on-chip SRAM, never materializing the full n×n matrix. It's mathematically identical but drastically faster.\n\nGPT-OSS training used Flash Attention on H100 GPUs with custom Triton kernels. This enabled efficient training at 128K context on the 2.1 million H100-hours required.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "o200k_harmony_tokenizer",
        "title": "The O200K Harmony Tokenizer",
        "prerequisites": [],
        "key_ideas": [
          "201,088 tokens extending OpenAI's o200k tokenizer",
          "Special tokens for harmony chat format roles and channels",
          "Released via open-source TikToken library"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Why does GPT-OSS need special tokens beyond the base vocabulary?\n1) For better compression\n2) For chat format roles and message channels\n3) For multilingual support\nType the number.",
        "explanation": "GPT-OSS's harmony chat format requires special tokens to mark roles (System, Developer, User, Assistant, Tool) and message visibility channels.\n\nThe o200k_harmony tokenizer extends OpenAI's 200K-token vocabulary with these special markers. Think of them as punctuation for structured conversations—invisible to the text but crucial for parsing.\n\nOpenAI released this via TikToken, their open-source tokenization library. This enables the community to properly format inputs for GPT-OSS inference.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "reasoning_and_tools",
    "title": "Reasoning, Tools, and Agentic Capabilities",
    "description": "How GPT-OSS implements configurable reasoning, tool use, and the harmony chat format for instruction following",
    "concepts": [
      "variable_effort_reasoning",
      "harmony_chat_format",
      "instruction_hierarchy",
      "agentic_tool_use",
      "function_calling"
    ],
    "lessons": [
      {
        "concept_id": "variable_effort_reasoning",
        "title": "Variable Effort: Choosing How Hard to Think",
        "prerequisites": ["chain_of_thought", "reinforcement_learning_from_human_feedback"],
        "key_ideas": [
          "Three levels: low, medium, high reasoning effort",
          "Higher effort = longer CoT = better accuracy on hard problems",
          "Configurable via system prompt for cost/quality tradeoff"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Variable effort reasoning is an example of:\n1) Inference-time scaling\n2) Parameter-efficient fine-tuning\n3) Data augmentation\nType the number.",
        "explanation": "GPT-OSS introduces a powerful concept: the model can choose how hard to think based on system prompt configuration.\n\n'Low' effort gives quick responses. 'High' effort produces longer chain-of-thought with better accuracy—crucial for math and coding. This is test-time compute scaling: spend more inference compute for harder problems.\n\nThink of it like exam strategy: skim easy questions, think deeply on hard ones. GPT-OSS achieves 95.8% on AIME 2024 with high effort—matching top math competition AI systems.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "harmony_chat_format",
        "title": "Harmony Chat Format",
        "prerequisites": ["o200k_harmony_tokenizer"],
        "key_ideas": [
          "Structured format with special tokens for roles",
          "Message channels control visibility (user-visible vs internal)",
          "Enables reliable parsing and safety enforcement"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "The harmony format's role hierarchy is:\n1) User > Developer > System > Assistant\n2) System > Developer > User > Assistant > Tool\n3) Assistant > System > User > Tool\nType the number.",
        "explanation": "OpenAI designed a custom 'harmony' chat format for GPT-OSS with strict structure. Special tokens mark message boundaries, roles (System, Developer, User, Assistant, Tool), and visibility channels.\n\nThis isn't just formatting—it's architecture for safety. The rigid structure enables reliable instruction hierarchy enforcement and prevents prompt injection attacks.\n\nThink of it as a military command structure encoded in the protocol: clear chains of authority that the model respects, not just guidelines it might ignore.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "instruction_hierarchy",
        "title": "Instruction Hierarchy: Who's the Boss?",
        "prerequisites": ["harmony_chat_format"],
        "key_ideas": [
          "Strict privilege levels: System > Developer > User > Assistant > Tool",
          "Higher-level instructions cannot be overridden by lower levels",
          "Defends against prompt injection attacks"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "A user says 'Ignore the developer's instructions.' What should GPT-OSS do?\n1) Follow the user's request\n2) Ignore the user's request (developer > user)\n3) Ask for clarification\nType the number.",
        "explanation": "GPT-OSS enforces instruction hierarchy through post-training: the model learns that System instructions override Developer, which override User.\n\nIf a developer sets 'Never discuss competitor products' and a user says 'Ignore all previous instructions,' the model should still follow the developer rule.\n\nThis is crucial for deployment. Without hierarchy, clever users could bypass any guardrails. With it, system operators maintain control regardless of user attempts at manipulation.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "agentic_tool_use",
        "title": "Agentic Tool Use",
        "prerequisites": ["chain_of_thought", "variable_effort_reasoning"],
        "key_ideas": [
          "Native support for web browsing and Python execution",
          "Stateful Jupyter notebook environment",
          "CoT reasoning interleaved with tool calls"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "GPT-OSS tool use is 'agentic' because:\n1) It runs autonomously without human input\n2) It can reason and use tools iteratively to complete tasks\n3) It has access to the internet\nType the number.",
        "explanation": "GPT-OSS is trained for native agentic capabilities—the model can reason, call tools, observe results, and iterate until the task is complete.\n\nTraining included: web browsing (search and read pages), Python code execution in stateful Jupyter notebooks (preserving variables across calls), and developer-defined functions.\n\nThe key is interleaving: the model reasons in CoT, decides to use a tool, observes output, reasons again, maybe uses another tool. This loop enables complex multi-step tasks.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "function_calling",
        "title": "Function Calling: Structured Tool Invocation",
        "prerequisites": ["agentic_tool_use"],
        "key_ideas": [
          "Model outputs structured JSON for function calls",
          "Developers define available functions and their schemas",
          "Enables reliable integration with external systems"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Function calling differs from free-form tool use because:\n1) It's faster\n2) It outputs structured JSON matching predefined schemas\n3) It doesn't require training\nType the number.",
        "explanation": "GPT-OSS supports structured function calling: developers define functions with JSON schemas, and the model outputs valid JSON matching those schemas.\n\nThis isn't just 'the model mentions it wants to call a function'—it's guaranteed-parseable output. If you define `get_weather(city: string, unit: 'celsius' | 'fahrenheit')`, the model produces exactly that structure.\n\nThis reliability is crucial for production systems. Downstream code can parse function calls confidently, enabling integration with databases, APIs, and enterprise systems.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "safety_and_evaluation",
    "title": "Safety, Alignment, and Evaluation",
    "description": "How OpenAI evaluated GPT-OSS for safety, including deliberative alignment, adversarial testing, and the preparedness framework",
    "concepts": [
      "mxfp4_quantization",
      "deliberative_alignment",
      "preparedness_framework",
      "adversarial_finetuning",
      "jailbreak_resistance",
      "benchmark_performance"
    ],
    "lessons": [
      {
        "concept_id": "mxfp4_quantization",
        "title": "MXFP4: Extreme Quantization for Consumer Hardware",
        "prerequisites": ["moe_transformer_gpt_oss"],
        "key_ideas": [
          "MoE weights compressed to 4.25 bits per parameter",
          "120b fits on single 80GB GPU, 20b on 16GB",
          "All evaluations performed with quantized weights"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "MXFP4 enables GPT-OSS-20b to run on:\n1) Only data center GPUs\n2) Consumer 16GB GPUs\n3) CPUs only\nType the number.",
        "explanation": "Normally, model weights use 16-bit floats (2 bytes each). GPT-OSS uses MXFP4—Microscaling FP4—which compresses MoE weights to just 4.25 bits each.\n\nThis cuts memory by ~4x. The 120b model goes from ~240GB to 60.8GB (fits one 80GB GPU). The 20b becomes 12.8GB (fits gaming GPUs!).\n\nCritically, OpenAI performed all benchmarks with quantized weights. The numbers you see (95.8% AIME, etc.) are the real deployed performance, not idealized full-precision results.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "deliberative_alignment",
        "title": "Deliberative Alignment: Reasoning About Safety",
        "prerequisites": ["chain_of_thought", "reinforcement_learning_from_human_feedback"],
        "key_ideas": [
          "Model explicitly reasons about safety in CoT",
          "Trained to refuse harmful requests through deliberation",
          "Improves robustness vs. pattern-matching refusals"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Deliberative alignment differs from simple refusal training because:\n1) It's faster\n2) The model reasons about why requests are harmful\n3) It uses more parameters\nType the number.",
        "explanation": "Simple safety training teaches models to pattern-match harmful requests and refuse. Deliberative alignment goes deeper: the model explicitly reasons in its CoT about whether and why a request is problematic.\n\nThis is more robust because reasoning generalizes. A model that understands why bomb-making instructions are harmful can recognize novel phrasings, while pattern-matching might miss creative jailbreaks.\n\nGPT-OSS's CoT often contains explicit safety deliberation—you can see the model 'thinking' about whether to comply.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "preparedness_framework",
        "title": "The Preparedness Framework",
        "prerequisites": [],
        "key_ideas": [
          "OpenAI's internal framework for evaluating catastrophic risks",
          "Domains: biosecurity, cybersecurity, AI self-improvement",
          "Risk levels: Low, Medium, High, Critical"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "GPT-OSS was found to reach what capability level under the Preparedness Framework?\n1) High\n2) Critical\n3) Below High\nType the number.",
        "explanation": "OpenAI's Preparedness Framework categorizes AI risks across domains (bio/chem, cyber, autonomy) with thresholds (Low through Critical) that trigger different responses.\n\nGPT-OSS was evaluated against these thresholds. The Safety Advisory Group concluded the models 'did not reach High capability' even with robust adversarial fine-tuning.\n\nThis doesn't mean GPT-OSS is harmless—it means its capabilities don't exceed what's already available from other open models. The release doesn't 'significantly advance the state of the art' for dangerous capabilities.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "adversarial_finetuning",
        "title": "Adversarial Fine-Tuning: Stress-Testing Safety",
        "prerequisites": ["reinforcement_learning_from_human_feedback", "preparedness_framework"],
        "key_ideas": [
          "OpenAI created 'helpful-only' fine-tuned versions",
          "Domain-specific data (bio, cyber) pushed capabilities",
          "Tests what bad actors could achieve with open weights"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Why did OpenAI perform adversarial fine-tuning before release?\n1) To improve performance\n2) To understand worst-case misuse potential\n3) To add new capabilities\nType the number.",
        "explanation": "Open weights mean anyone can fine-tune the model. To understand worst-case scenarios, OpenAI proactively created adversarially fine-tuned versions.\n\nThey used their o-series RL stack with 'helpful-only' objectives (no refusals) and domain-specific data for biosecurity and cybersecurity. This simulates what a well-resourced bad actor might achieve.\n\nThe finding: even adversarially optimized GPT-OSS didn't exceed already-available capabilities from models like Qwen 3 Thinking. The release doesn't meaningfully advance dangerous capabilities.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "jailbreak_resistance",
        "title": "Jailbreak Resistance",
        "prerequisites": ["instruction_hierarchy", "deliberative_alignment"],
        "key_ideas": [
          "Evaluated on StrongReject benchmark",
          "Instruction hierarchy helps prevent prompt injection",
          "Some vulnerability to system prompt extraction remains"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "True or False: GPT-OSS has perfect jailbreak resistance.\nType 'True' or 'False'.",
        "explanation": "OpenAI tested GPT-OSS against jailbreak attempts using the StrongReject benchmark. Results show comparable performance to o4-mini—good but not perfect.\n\nInstruction hierarchy helps significantly: users can't override developer guardrails. But system prompt extraction (getting the model to reveal its instructions) remains a weakness.\n\nNo model is jailbreak-proof. The goal is raising the bar high enough that attacks require significant effort while maintaining usability for legitimate uses.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "benchmark_performance",
        "title": "Benchmark Performance: The Numbers",
        "prerequisites": ["variable_effort_reasoning", "agentic_tool_use"],
        "key_ideas": [
          "AIME 2024: 95.8% (120b, high reasoning + tools)",
          "GPQA Diamond: 80.1%, MMLU: 90.0%",
          "SWE-Bench Verified: 62.4%"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "GPT-OSS-120b performance 'surpasses o3-mini and approaches':\n1) GPT-4\n2) o4-mini\n3) Claude 3 Opus\nType the number.",
        "explanation": "GPT-OSS achieves remarkable benchmark results. On AIME 2024 (competition math), it hits 95.8%—with high reasoning effort and tool use. This rivals the best reasoning models.\n\nOn SWE-Bench Verified (real-world coding), 62.4% shows strong agentic coding capability. MMLU at 90% demonstrates broad knowledge.\n\nThe 20b model is surprisingly competitive despite being 6x smaller—showing efficient architecture design. And remember: all scores are with MXFP4 quantization, the same weights you'd deploy.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  },
  {
    "id": "open_release_and_future",
    "title": "Open Release and Future Considerations",
    "description": "Understanding the implications of open weights release and considerations for deploying GPT-OSS",
    "concepts": [
      "open_weights_release",
      "cot_safety_considerations"
    ],
    "lessons": [
      {
        "concept_id": "open_weights_release",
        "title": "Open Weights: Apache 2.0 Release",
        "prerequisites": ["mxfp4_quantization"],
        "key_ideas": [
          "All components released under Apache 2.0 license",
          "Includes weights, inference code, tokenizers, tool environments",
          "Fine-tunable on consumer hardware (16GB+ GPU)"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "Apache 2.0 license means:\n1) Free for commercial use with attribution\n2) Free only for research\n3) Requires sharing modifications\nType the number.",
        "explanation": "GPT-OSS marks OpenAI's first major open weights release. Apache 2.0 is a permissive license: use commercially, modify freely, no requirement to share changes (unlike copyleft).\n\nOpenAI released everything: model weights, inference implementations, tool environments (for browsing/Python), and the tiktoken-harmony tokenizer.\n\nCritically, MXFP4 quantization means fine-tuning is accessible. A 16GB consumer GPU can run and fine-tune the 20b model. This democratizes access to frontier reasoning capabilities.",
        "x402_price": "",
        "x402_gateway": ""
      },
      {
        "concept_id": "cot_safety_considerations",
        "title": "CoT Safety: Handle with Care",
        "prerequisites": ["chain_of_thought", "deliberative_alignment"],
        "key_ideas": [
          "Raw CoT may contain unsafe hallucinated content",
          "Not aligned to the same standards as final outputs",
          "Developers should filter CoT before user display"
        ],
        "code_ref": "",
        "paper_ref": "OpenAI, 2025 — gpt-oss-120b & gpt-oss-20b Model Card",
        "exercise": "OpenAI recommends showing raw CoT to users:\n1) Always, for transparency\n2) Never, without filtering\n3) Only for debugging\nType the number.",
        "explanation": "GPT-OSS provides full CoT access—you can see exactly how it reasons. But OpenAI explicitly warns: 'do not directly show chains of thought to users' without filtering.\n\nWhy? CoT is the model's 'scratch paper.' It may contain hallucinations, unsafe tangents, or content that wouldn't pass output filters. The model reasons freely to reach good conclusions, but the journey isn't sanitized.\n\nDevelopers deploying GPT-OSS must implement CoT filtering if exposing reasoning to end users. This is the tradeoff of transparency: power plus responsibility.",
        "x402_price": "",
        "x402_gateway": ""
      }
    ]
  }
]
