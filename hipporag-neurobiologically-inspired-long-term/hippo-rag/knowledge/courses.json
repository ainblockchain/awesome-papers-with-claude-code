[{"id":"stage1_foundations","title":"Memory & Retrieval Foundations","description":"Neuroscience theory behind HippoRAG and standard RAG basics.","concepts":["rag_basics","hippocampal_indexing"],"lessons":[{"concept_id":"rag_basics","title":"What Is RAG?","prerequisites":[],"key_ideas":["retrieve then read","grounding hallucinations"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"What problem does RAG solve for LLMs?\n1) It trains the model on new data\n2) It grounds generation with retrieved external knowledge\n3) It compresses the model weights\nAnswer with a number.","explanation":"RAG augments an LLM at inference time by retrieving relevant passages from a corpus, letting the model answer questions about knowledge it never saw during training.","x402_price":"","x402_gateway":""},{"concept_id":"hippocampal_indexing","title":"The Brain's Filing System","prerequisites":["rag_basics"],"key_ideas":["hippocampus as index","neocortex as encoder"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"In hippocampal indexing theory, what role does the hippocampus play?\n1) It stores raw sensory data\n2) It maintains associative indices binding neocortical patterns\n3) It generates language output\nAnswer with a number.","explanation":"The hippocampus does not store memories directly; it holds an index of pointers that bind distributed neocortical representations, enabling fast associative recall from partial cues.","x402_price":"","x402_gateway":""}]},{"id":"stage2_core","title":"Building the Artificial Hippocampus","description":"How HippoRAG constructs and queries its knowledge graph index.","concepts":["openIE","knowledge_graph","synonymy_edges","personalized_pagerank","node_specificity"],"lessons":[{"concept_id":"openIE","title":"Turning Text into Triples","prerequisites":["hippocampal_indexing"],"key_ideas":["subject-relation-object","LLM extraction"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"OpenIE extracts knowledge in what form?\n1) Embedding vectors\n2) (subject, relation, object) triples\n3) SQL tables\nAnswer with a number.","explanation":"HippoRAG uses an instruction-tuned LLM to extract schemaless triples from each passage, turning unstructured text into graph nodes (noun phrases) and edges (semantic relations).","x402_price":"","x402_gateway":""},{"concept_id":"knowledge_graph","title":"The Hippocampal Index","prerequisites":["openIE"],"key_ideas":["corpus-wide graph","associative links"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"True or False: HippoRAG's knowledge graph is rebuilt fresh for every query.\nAnswer True or False.","explanation":"The KG is built offline during indexing and reused across all queries, making retrieval cheap; it acts as a persistent associative index over the entire corpus.","x402_price":"","x402_gateway":""},{"concept_id":"synonymy_edges","title":"Connecting the Dots with Synonymy","prerequisites":["knowledge_graph"],"key_ideas":["embedding similarity","threshold 0.8"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"What is the purpose of synonymy edges in HippoRAG?\n1) Speed up PageRank computation\n2) Link semantically similar nodes to handle coreference\n3) Reduce graph storage size\nAnswer with a number.","explanation":"A retrieval encoder computes similarity between all node pairs; pairs above the 0.8 threshold receive a synonymy edge, allowing the graph to connect mentions of the same entity even when phrased differently.","x402_price":"","x402_gateway":""},{"concept_id":"personalized_pagerank","title":"One-Step Multi-hop with PageRank","prerequisites":["knowledge_graph","synonymy_edges"],"key_ideas":["seed query nodes","probability propagation"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"Why does HippoRAG use Personalized PageRank instead of plain PageRank?\n1) It is faster to compute\n2) It biases the walk toward query-relevant nodes\n3) It works on directed graphs only\nAnswer with a number.","explanation":"PPR initializes the random walk distribution at query-named-entity nodes, so probability mass flows only through graph neighborhoods relevant to the query, achieving multi-hop reasoning in a single pass.","x402_price":"","x402_gateway":""},{"concept_id":"node_specificity","title":"Weighting What Matters","prerequisites":["personalized_pagerank"],"key_ideas":["inverse passage count","local IDF"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"Node specificity down-weights nodes that appear in many passages. Which classic NLP concept is this analogous to?\n1) TF-IDF's IDF component\n2) BM25's term frequency\n3) Cosine similarity\nAnswer with a number.","explanation":"Node specificity = 1 / (number of passages containing the node), a local analogue of inverse document frequency that prevents common, uninformative nodes from dominating PPR scores.","x402_price":"","x402_gateway":""}]},{"id":"stage3_frontier","title":"Performance, Limits & Beyond","description":"Empirical results, failure modes, and open research directions.","concepts":["multi_hop_qa","hipporag_limits"],"lessons":[{"concept_id":"multi_hop_qa","title":"Beating the Multi-hop Benchmark","prerequisites":["personalized_pagerank"],"key_ideas":["MuSiQue","2WikiMultiHopQA","20% gain"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"HippoRAG achieves multi-hop retrieval performance comparable to iterative methods like IRCoT while being how much cheaper?\n1) 2-3× cheaper\n2) 10-30× cheaper\n3) About the same cost\nAnswer with a number.","explanation":"On MuSiQue and 2WikiMultiHopQA, HippoRAG single-step retrieval matches iterative chain-of-thought retrieval (IRCoT) while costing 10-30× less by replacing repeated LLM calls with one PPR graph traversal.","x402_price":"","x402_gateway":""},{"concept_id":"hipporag_limits","title":"What HippoRAG Can't Do Yet","prerequisites":["multi_hop_qa"],"key_ideas":["LLM extraction cost","dynamic updates"],"code_ref":"","paper_ref":"Jiménez Gutiérrez et al., 2024 — HippoRAG","exercise":"Which of these is a stated limitation of HippoRAG?\n1) It cannot handle English text\n2) The offline indexing phase requires LLM-based OpenIE which adds upfront cost\n3) It only works on single-hop questions\nAnswer with a number.","explanation":"Building the KG requires calling an LLM for OpenIE on every document, incurring upfront indexing cost; the paper also notes the index is static and does not yet support efficient online updates as corpora grow.","x402_price":"","x402_gateway":""}]}]
