# Llama 2: Open Foundation and Fine-Tuned Chat Models Learning Path

A Claude Code-powered interactive learning path based on
"Llama 2: Open Foundation and Fine-Tuned Chat Models" by Touvron et al., 2023.

## Getting Started

1. Open Claude Code in this directory:
   ```bash
   cd understanding-llama-2
   claude
   ```

2. Start learning â€” just chat naturally:
   ```
   explore              # see the knowledge graph
   teach me <concept>   # start a lesson
   give me a challenge  # get a quiz
   done                 # mark complete, move on
   ```

## Sharing Progress with Friends

1. Create your learner branch:
   ```bash
   git checkout -b learner/your-name
   ```

2. Commit progress as you learn:
   ```bash
   git add .learner/
   git commit -m "Progress update"
   git push origin learner/your-name
   ```

3. Fetch friends' branches:
   ```bash
   git fetch --all
   friends
   ```

## Course Structure

- **Foundations of Large Language Models** (4 concepts): Learn the core concepts behind LLMs: transformers, autoregressive modeling, and pretraining at scale.
- **Training & Fine-tuning Llama 2** (5 concepts): Explore supervised fine-tuning, RLHF, and techniques to make Llama 2 follow instructions and have helpful conversations.
- **Advanced Techniques & Safety** (4 concepts): Dive into safety training, red teaming, dialogue optimization, and inference efficiency.
- **Open Source & Responsible AI** (2 concepts): Explore the significance of open-source LLMs and the principles behind responsible AI development.

## Stats

- **15** concepts across **4** courses
- **4** foundational, **5** intermediate, **4** advanced, **2** frontier concepts

## Key Topics

- Transformer architecture and self-attention
- Autoregressive pretraining on 2 trillion tokens
- Supervised fine-tuning (SFT) and RLHF
- Safety training and red teaming
- Multi-turn dialogue optimization
- Open-source LLMs and responsible AI

---

**Paper**: Llama 2: Open Foundation and Fine-Tuned Chat Models  
**Authors**: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, et al.  
**Published**: July 2023 (arXiv:2307.09288)
